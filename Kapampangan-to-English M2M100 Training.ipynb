{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b743eb",
   "metadata": {},
   "source": [
    "# === Kapampangan-to-English M2M100 Training Pipeline ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06d12ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    M2M100ForConditionalGeneration\n",
    ")\n",
    "from transformers.trainer_seq2seq import Seq2SeqTrainer\n",
    "from transformers.training_args_seq2seq import Seq2SeqTrainingArguments\n",
    "from transformers.data.data_collator import DataCollatorForSeq2Seq\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df05eac",
   "metadata": {},
   "source": [
    "# === 1. Config ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01e2dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Config ===\n",
    "CSV_PATH = \"data/kapampangan_english.csv\"\n",
    "MODEL_NAME = \"facebook/m2m100_418M\"\n",
    "MODEL_DIR = \"./kapampangan_mt_model\"\n",
    "\n",
    "SPECIAL_SRC_TOKEN = \"<kap>\"   # Kapampangan marker\n",
    "TGT_LANG = \"en\"               # English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145e560",
   "metadata": {},
   "source": [
    "# === 2. Load CSV ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7781ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "df = df.rename(columns={\"kapampangan\": \"src_text\", \"english\": \"tgt_text\"})\n",
    "df = df.dropna(subset=[\"src_text\", \"tgt_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624412d3",
   "metadata": {},
   "source": [
    "# === 3. Convert to HF Dataset ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3902dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df[[\"src_text\", \"tgt_text\"]])\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74755110",
   "metadata": {},
   "source": [
    "# === 4. Load Tokenizer & Model ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa2b10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9eef0c465b47b48bf30e6b887bf942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  88%|########8 | 1.71G/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "M2M100ScaledWordEmbedding(128105, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PC\\.cache\\huggingface\\hub\\models--facebook--m2m100_418M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add <kap> as a special token\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': [SPECIAL_SRC_TOKEN]})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a70879",
   "metadata": {},
   "source": [
    "# === 5. Preprocess ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8fc85d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5a00f056854543ab586618cde0a6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925c4202d5d344b2a0a067d0392d83f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/232 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(examples):\n",
    "    # Prepend <kap> to source text\n",
    "    src_texts = [f\"{SPECIAL_SRC_TOKEN} {text}\" for text in examples[\"src_text\"]]\n",
    "\n",
    "    # Tokenize source\n",
    "    model_inputs = tokenizer(\n",
    "        src_texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "    # Tokenize target\n",
    "    labels = tokenizer(\n",
    "        examples[\"tgt_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    # Replace pad token IDs with -100\n",
    "    labels = [[(t if t != tokenizer.pad_token_id else -100) for t in label] for label in labels]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad32329",
   "metadata": {},
   "source": [
    "# === 6. Training Args ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab028ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # Replace -100 with pad_token_id so metric can handle it\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    accuracy = metric_acc.compute(predictions=predictions, references=labels)\n",
    "    return {\"accuracy\": accuracy[\"accuracy\"]}\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=MODEL_DIR,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    eval_steps=500,       # evaluate every 500 steps\n",
    "    logging_steps=500,    # log every 500 steps\n",
    "    save_steps=500,       # save every 500 steps\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fee01b",
   "metadata": {},
   "source": [
    "# === 7. Trainer ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94460fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_19320\\2430946255.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534952ea",
   "metadata": {},
   "source": [
    "# === 8. Train ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce654d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='3480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/3480 00:33 < 16:11:15, 0.06 it/s, Epoch 0.01/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f638c711",
   "metadata": {},
   "source": [
    "# === 9. Save ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dbd831",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(MODEL_DIR)\n",
    "tokenizer.save_pretrained(MODEL_DIR)\n",
    "print(f\"✅ Model saved to: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2696e28",
   "metadata": {},
   "source": [
    "# === 10. Translation Function ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf1ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kapampangan_translate(text):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    src_text = f\"{SPECIAL_SRC_TOKEN} {text}\"\n",
    "    inputs = tokenizer(src_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.get_lang_id(TGT_LANG)  # Force English output\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b30ff",
   "metadata": {},
   "source": [
    "# === 11. Evaluate BLEU ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119612dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Evaluating BLEU Score ---\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "preds = [kapampangan_translate(x) for x in df[\"src_text\"]]\n",
    "refs = [[x] for x in df[\"tgt_text\"]]\n",
    "\n",
    "bleu_score = bleu.compute(predictions=preds, references=refs)\n",
    "print(\" BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec37f9",
   "metadata": {},
   "source": [
    "# === 12. Manual Test ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9cbbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Manual Test ---\")\n",
    "sample_texts = [\n",
    "    \"Ali ku balu\",\n",
    "    \"Anya ka?\",\n",
    "    \"Masanting ya ing panaun ngeni\",\n",
    "    \"E ku makanyan\",\n",
    "]\n",
    "\n",
    "for i, kap_text in enumerate(sample_texts):\n",
    "    translated = kapampangan_translate(kap_text)\n",
    "    print(f\"[{i+1}] Kapampangan: {kap_text}\")\n",
    "    print(f\"    ➤ English: {translated}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

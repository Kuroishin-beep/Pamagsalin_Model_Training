{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0920e692",
   "metadata": {},
   "source": [
    "# === Kapampangan-to-English NLLB-200 Training Pipeline (with <kap> tag) ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbb73aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM\n",
    ")\n",
    "from transformers.trainer_seq2seq import Seq2SeqTrainer\n",
    "from transformers.training_args_seq2seq import Seq2SeqTrainingArguments\n",
    "from transformers.data.data_collator import DataCollatorForSeq2Seq\n",
    "import torch\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57188df0",
   "metadata": {},
   "source": [
    "# === 1. Config ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec6a97df",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"data/kapampangan_english.csv\"\n",
    "MODEL_NAME = \"facebook/nllb-200-distilled-600M\"  # NLLB model\n",
    "MODEL_DIR = \"./kapampangan_mt_nllb\"\n",
    "\n",
    "SPECIAL_SRC_TOKEN = \"<kap>\"   \n",
    "TGT_LANG = \"eng_Latn\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215a3088",
   "metadata": {},
   "source": [
    "# === 2. Load CSV ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0206e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "df = df.rename(columns={\"kapampangan\": \"src_text\", \"english\": \"tgt_text\"})\n",
    "df = df.dropna(subset=[\"src_text\", \"tgt_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa01e85",
   "metadata": {},
   "source": [
    "# === 3. Convert to HF Dataset ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db28d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df[[\"src_text\", \"tgt_text\"]])\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb4569c",
   "metadata": {},
   "source": [
    "# === 4. Load Tokenizer & Model ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "efd264c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2M100ScaledWordEmbedding(256205, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, src_lang=\"eng_Latn\")  # temp placeholder\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add <kap> as a special token\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': [SPECIAL_SRC_TOKEN]})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50c0a5",
   "metadata": {},
   "source": [
    "# === 5. Preprocess ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b26a7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['src_text', 'tgt_text'], dtype='object')\n",
      "          src_text  tgt_text\n",
      "0          Sakoru!     Help!\n",
      "1            Tuto?   Really?\n",
      "2  Kutang mu kaya.  Ask him.\n",
      "3     Mako ka ken.  Go away.\n",
      "4        Yaku man.  Me, too.\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(df.head())\n",
    "df = df.rename(columns={\"kapampangan\": \"src_text\", \"english\": \"tgt_text\"})\n",
    "df = df.dropna(subset=[\"src_text\", \"tgt_text\"])\n",
    "df = df[df[\"src_text\"].str.strip() != \"\"]\n",
    "df = df[df[\"tgt_text\"].str.strip() != \"\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0e991c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0ce3c8512d4dcbb1f8652273c19b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m     model_inputs[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m] = labels_input_ids\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_inputs\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m tokenized_dataset = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msrc_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtgt_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\dataset_dict.py:946\u001b[39m, in \u001b[36mDatasetDict.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\u001b[39m\n\u001b[32m    943\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    944\u001b[39m     function = bind(function, split)\n\u001b[32m--> \u001b[39m\u001b[32m946\u001b[39m dataset_dict[split] = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    968\u001b[39m     function = function.func\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py:560\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    553\u001b[39m self_format = {\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    555\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    558\u001b[39m }\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py:3318\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3316\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3317\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m unprocessed_kwargs \u001b[38;5;129;01min\u001b[39;00m unprocessed_kwargs_per_job:\n\u001b[32m-> \u001b[39m\u001b[32m3318\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munprocessed_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3319\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3321\u001b[39m \u001b[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py:3674\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3672\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3673\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3674\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_examples_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3676\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py:3624\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3622\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3623\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3624\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py:3547\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3545\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3546\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3547\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3548\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mpreprocess\u001b[39m\u001b[34m(examples)\u001b[39m\n\u001b[32m      9\u001b[39m model_inputs = tokenizer(\n\u001b[32m     10\u001b[39m     src_texts,\n\u001b[32m     11\u001b[39m     truncation=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     12\u001b[39m     padding=\u001b[33m\"\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m     max_length=\u001b[32m128\u001b[39m\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Tokenize target (English)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m labels = \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtgt_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_length\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\n\u001b[32m     22\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Replace pad token IDs in labels with -100\u001b[39;00m\n\u001b[32m     25\u001b[39m labels_input_ids = [\n\u001b[32m     26\u001b[39m     [(t \u001b[38;5;28;01mif\u001b[39;00m t != tokenizer.pad_token_id \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m100\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m label]\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     28\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2857\u001b[39m, in \u001b[36m__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2827\u001b[39m \u001b[38;5;129m@add_end_docstrings\u001b[39m(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n\u001b[32m   2828\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m   2829\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2852\u001b[39m     **kwargs,\n\u001b[32m   2853\u001b[39m ) -> BatchEncoding:\n\u001b[32m   2854\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2855\u001b[39m \u001b[33;03m    Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\u001b[39;00m\n\u001b[32m   2856\u001b[39m \u001b[33;03m    sequences.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2857\u001b[39m \n\u001b[32m   2858\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   2859\u001b[39m \u001b[33;03m        text (`str`, `list[str]`, `list[list[str]]`, *optional*):\u001b[39;00m\n\u001b[32m   2860\u001b[39m \u001b[33;03m            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\u001b[39;00m\n\u001b[32m   2861\u001b[39m \u001b[33;03m            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\u001b[39;00m\n\u001b[32m   2862\u001b[39m \u001b[33;03m            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\u001b[39;00m\n\u001b[32m   2863\u001b[39m \u001b[33;03m        text_pair (`str`, `list[str]`, `list[list[str]]`, *optional*):\u001b[39;00m\n\u001b[32m   2864\u001b[39m \u001b[33;03m            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\u001b[39;00m\n\u001b[32m   2865\u001b[39m \u001b[33;03m            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\u001b[39;00m\n\u001b[32m   2866\u001b[39m \u001b[33;03m            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\u001b[39;00m\n\u001b[32m   2867\u001b[39m \u001b[33;03m        text_target (`str`, `list[str]`, `list[list[str]]`, *optional*):\u001b[39;00m\n\u001b[32m   2868\u001b[39m \u001b[33;03m            The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\u001b[39;00m\n\u001b[32m   2869\u001b[39m \u001b[33;03m            list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\u001b[39;00m\n\u001b[32m   2870\u001b[39m \u001b[33;03m            you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\u001b[39;00m\n\u001b[32m   2871\u001b[39m \u001b[33;03m        text_pair_target (`str`, `list[str]`, `list[list[str]]`, *optional*):\u001b[39;00m\n\u001b[32m   2872\u001b[39m \u001b[33;03m            The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\u001b[39;00m\n\u001b[32m   2873\u001b[39m \u001b[33;03m            list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\u001b[39;00m\n\u001b[32m   2874\u001b[39m \u001b[33;03m            you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\u001b[39;00m\n\u001b[32m   2875\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2876\u001b[39m     \u001b[38;5;66;03m# To avoid duplicating\u001b[39;00m\n\u001b[32m   2877\u001b[39m     all_kwargs = {\n\u001b[32m   2878\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33madd_special_tokens\u001b[39m\u001b[33m\"\u001b[39m: add_special_tokens,\n\u001b[32m   2879\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpadding\u001b[39m\u001b[33m\"\u001b[39m: padding,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2894\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mverbose\u001b[39m\u001b[33m\"\u001b[39m: verbose,\n\u001b[32m   2895\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\nllb\\tokenization_nllb_fast.py:260\u001b[39m, in \u001b[36mNllbTokenizerFast._switch_to_target_mode\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_switch_to_target_mode\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_tgt_lang_special_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtgt_lang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\nllb\\tokenization_nllb_fast.py:290\u001b[39m, in \u001b[36mNllbTokenizerFast.set_tgt_lang_special_tokens\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_tgt_lang_special_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    286\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Reset the special tokens to the target lang setting.\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[33;03m    - In legacy mode: No prefix and suffix=[eos, tgt_lang_code].\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[33;03m    - In default mode: Prefix=[tgt_lang_code], suffix = [eos]\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     \u001b[38;5;28mself\u001b[39m.cur_lang_code = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_tokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.legacy_behaviour:\n\u001b[32m    292\u001b[39m         \u001b[38;5;28mself\u001b[39m.prefix_tokens = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_fast.py:368\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.convert_tokens_to_ids\u001b[39m\u001b[34m(self, tokens)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokens, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._convert_token_to_id_with_added_voc(tokens)\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_token_to_id_with_added_voc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "def preprocess(examples):\n",
    "    src_texts = [f\"{SPECIAL_SRC_TOKEN} {text}\" if text is not None else f\"{SPECIAL_SRC_TOKEN} \" \n",
    "                 for text in examples[\"src_text\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        src_texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        text_target=[t if t is not None else \"\" for t in examples[\"tgt_text\"]],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "    labels_input_ids = [\n",
    "        [(t if t != tokenizer.pad_token_id else -100) for t in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "    model_inputs[\"labels\"] = labels_input_ids\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760f518c",
   "metadata": {},
   "source": [
    "# === 6. Training Args ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "574bfdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=MODEL_DIR,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    eval_steps=500,       # evaluate every 500 steps\n",
    "    logging_steps=500,    # log every 500 steps\n",
    "    save_steps=500,       # save every 500 steps\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d853e4",
   "metadata": {},
   "source": [
    "# === 7. Trainer ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f79487ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n\u001b[32m      2\u001b[39m trainer = Seq2SeqTrainer(\n\u001b[32m      3\u001b[39m     model=model,\n\u001b[32m      4\u001b[39m     args=training_args,\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     train_dataset=\u001b[43mtokenized_dataset\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      6\u001b[39m     eval_dataset=tokenized_dataset[\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      7\u001b[39m     data_collator=data_collator,\n\u001b[32m      8\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenized_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94068b3b",
   "metadata": {},
   "source": [
    "# === 8. Train ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dd79ae",
   "metadata": {},
   "source": [
    "# === 9. Save ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c64989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(MODEL_DIR)\n",
    "tokenizer.save_pretrained(MODEL_DIR)\n",
    "print(f\"✅ Model saved to: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55379746",
   "metadata": {},
   "source": [
    "# === 10. Translation Function ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ccacd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kapampangan_translate(text):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    src_text = f\"{SPECIAL_SRC_TOKEN} {text}\"\n",
    "    inputs = tokenizer(src_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(TGT_LANG)  # Force English output\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ea2d2",
   "metadata": {},
   "source": [
    "# === 11. Evaluate BLEU ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d444b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Evaluating BLEU Score ---\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "preds = [kapampangan_translate(x) for x in df[\"src_text\"]]\n",
    "refs = [[x] for x in df[\"tgt_text\"]]\n",
    "\n",
    "bleu_score = bleu.compute(predictions=preds, references=refs)\n",
    "print(\" BLEU Score:\", bleu_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83331cbf",
   "metadata": {},
   "source": [
    "# === 12. Manual Test ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e8f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Manual Test ---\")\n",
    "sample_texts = [\n",
    "    \"Ali ku balu\",\n",
    "    \"Anya ka?\",\n",
    "    \"Masanting ya ing panaun ngeni\",\n",
    "    \"E ku makanyan\",\n",
    "]\n",
    "\n",
    "for i, kap_text in enumerate(sample_texts):\n",
    "    translated = kapampangan_translate(kap_text)\n",
    "    print(f\"[{i+1}] Kapampangan: {kap_text}\")\n",
    "    print(f\"    ➤ English: {translated}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

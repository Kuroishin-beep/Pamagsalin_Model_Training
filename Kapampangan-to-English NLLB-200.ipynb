{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0920e692",
   "metadata": {},
   "source": [
    "# === Kapampangan-to-English NLLB-200 Training Pipeline (with <kap> tag) ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb73aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57188df0",
   "metadata": {},
   "source": [
    "# === 1. Config ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6a97df",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"data/kapampangan_english.csv\"\n",
    "MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
    "MODEL_DIR = \"./kapampangan_mt_nllb\"\n",
    "\n",
    "SPECIAL_SRC_TOKEN = \"<kap>\"\n",
    "TGT_LANG = \"eng_Latn\"   # English (Latin script)\n",
    "\n",
    "EXTRA_TOKENS = [\"banua\", \"masanting\", \"eku\", \"ala\", \"nung\", \"manyaman\"]       # English (Latin script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cf75adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['English', 'Kapampangan'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215a3088",
   "metadata": {},
   "source": [
    "# === 2. Load CSV ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0206e139",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "['src_text', 'tgt_text']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_20556\\2026338930.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m df = pd.read_csv(CSV_PATH)\n\u001b[32m      2\u001b[39m df = df.rename(columns={\u001b[33m\"kapampangan\"\u001b[39m: \u001b[33m\"src_text\"\u001b[39m, \u001b[33m\"english\"\u001b[39m: \u001b[33m\"tgt_text\"\u001b[39m})\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = df.dropna(subset=[\u001b[33m\"src_text\"\u001b[39m, \u001b[33m\"tgt_text\"\u001b[39m])\n\u001b[32m      4\u001b[39m df.head()\n",
      "\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[39m\n\u001b[32m   6666\u001b[39m             ax = self._get_axis(agg_axis)\n\u001b[32m   6667\u001b[39m             indices = ax.get_indexer_for(subset)\n\u001b[32m   6668\u001b[39m             check = indices == -\u001b[32m1\u001b[39m\n\u001b[32m   6669\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m check.any():\n\u001b[32m-> \u001b[39m\u001b[32m6670\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m KeyError(np.array(subset)[check].tolist())\n\u001b[32m   6671\u001b[39m             agg_obj = self.take(indices, axis=agg_axis)\n\u001b[32m   6672\u001b[39m \n\u001b[32m   6673\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m thresh \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m lib.no_default:\n",
      "\u001b[31mKeyError\u001b[39m: ['src_text', 'tgt_text']"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "df = df.rename(columns={\"kapampangan\": \"src_text\", \"english\": \"tgt_text\"})\n",
    "df = df.dropna(subset=[\"src_text\", \"tgt_text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa01e85",
   "metadata": {},
   "source": [
    "# === 3. Convert to HF Dataset ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db28d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df[[\"src_text\", \"tgt_text\"]])\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "def synonym_replacement_en(text, prob=0.1):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        if random.random() < prob:\n",
    "            syns = wordnet.synsets(w)\n",
    "            if syns:\n",
    "                lemmas = syns[0].lemma_names()\n",
    "                if lemmas:\n",
    "                    w = random.choice(lemmas)\n",
    "        new_words.append(w)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def noise_injection_kap(text, prob=0.02):\n",
    "    chars = list(text)\n",
    "    for i in range(len(chars)):\n",
    "        if random.random() < prob:\n",
    "            op = random.choice([\"swap\",\"delete\",\"insert\"])\n",
    "            if op == \"swap\" and i < len(chars)-1:\n",
    "                chars[i], chars[i+1] = chars[i+1], chars[i]\n",
    "            elif op == \"delete\":\n",
    "                chars[i] = \"\"\n",
    "            elif op == \"insert\":\n",
    "                chars[i] = chars[i] + random.choice(\"aeiou\")\n",
    "    return \"\".join(chars)\n",
    "\n",
    "def word_dropout(text, prob=0.05):\n",
    "    words = text.split()\n",
    "    return \" \".join([w for w in words if random.random() > prob])\n",
    "\n",
    "def augment_pair(src, tgt):\n",
    "    src = noise_injection_kap(src, prob=0.02)\n",
    "    src = word_dropout(src, prob=0.05)\n",
    "    tgt = synonym_replacement_en(tgt, prob=0.05)\n",
    "    return src, tgt\n",
    "\n",
    "def augment_dataset(dataset, multiplier=2):\n",
    "    augmented = {\"src_text\": [], \"tgt_text\": []}\n",
    "    for ex in dataset:\n",
    "        src, tgt = ex[\"src_text\"], ex[\"tgt_text\"]\n",
    "        # keep original\n",
    "        augmented[\"src_text\"].append(src)\n",
    "        augmented[\"tgt_text\"].append(tgt)\n",
    "        # add augmented samples\n",
    "        for _ in range(multiplier):\n",
    "            new_src, new_tgt = augment_pair(src, tgt)\n",
    "            augmented[\"src_text\"].append(new_src)\n",
    "            augmented[\"tgt_text\"].append(new_tgt)\n",
    "    return Dataset.from_dict(augmented)\n",
    "\n",
    "# Only augment training set\n",
    "dataset[\"train\"] = augment_dataset(dataset[\"train\"], multiplier=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb4569c",
   "metadata": {},
   "source": [
    "# === 4. Load Tokenizer & Model ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd264c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add special tokens\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': [SPECIAL_SRC_TOKEN] + EXTRA_TOKENS})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Force English output\n",
    "model.config.forced_bos_token_id = tokenizer.convert_tokens_to_ids(TGT_LANG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50c0a5",
   "metadata": {},
   "source": [
    "# === 5. Preprocess ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b26a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    src_texts = [f\"{SPECIAL_SRC_TOKEN} {text}\" for text in examples[\"src_text\"]]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        src_texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        examples[\"tgt_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    labels = [[(t if t != tokenizer.pad_token_id else -100) for t in label] for label in labels]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760f518c",
   "metadata": {},
   "source": [
    "# === 6. Training Args ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574bfdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_bleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = metric_bleu.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
    "    return {\"sacrebleu\": result[\"score\"]}\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=MODEL_DIR,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=30,\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=200,\n",
    "    logging_steps=50,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d853e4",
   "metadata": {},
   "source": [
    "# === 7. Trainer ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79487ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94068b3b",
   "metadata": {},
   "source": [
    "# === 8. Train ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dd79ae",
   "metadata": {},
   "source": [
    "# === 9. Save ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c64989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(MODEL_DIR)\n",
    "tokenizer.save_pretrained(MODEL_DIR)\n",
    "print(f\"✅ Model saved to: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55379746",
   "metadata": {},
   "source": [
    "# === 10. Translation Function ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ccacd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_translate(texts, batch_size=8):\n",
    "    results = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        src_texts = [f\"{SPECIAL_SRC_TOKEN} {t}\" for t in texts[i:i+batch_size]]\n",
    "        inputs = tokenizer(src_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs)\n",
    "        results.extend(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ea2d2",
   "metadata": {},
   "source": [
    "# === 11. Evaluate BLEU ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d444b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Evaluating BLEU Score ---\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "test_df = dataset[\"test\"].to_pandas()\n",
    "preds = batch_translate(test_df[\"src_text\"].tolist())\n",
    "refs = [[x] for x in test_df[\"tgt_text\"].tolist()]\n",
    "\n",
    "bleu_score = bleu.compute(predictions=preds, references=refs)\n",
    "print(\" BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83331cbf",
   "metadata": {},
   "source": [
    "# === 12. Manual Test ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e8f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Manual Test ---\")\n",
    "sample_texts = [\n",
    "    \"Ali ku balu\",\n",
    "    \"Anya ka?\",\n",
    "    \"Masanting ya ing panaun ngeni\",\n",
    "    \"E ku makanyan\",\n",
    "]\n",
    "\n",
    "for i, kap_text in enumerate(sample_texts):\n",
    "    translated = batch_translate([kap_text])[0]\n",
    "    print(f\"[{i+1}] Kapampangan: {kap_text}\")\n",
    "    print(f\"    ➤ English: {translated}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Data Config
save_data: runs/data_index

data:
  corpus_1:
    path_src: data/onmt/train.kap
    path_tgt: data/onmt/train.eng
  valid:
    path_src: data/onmt/valid.kap
    path_tgt: data/onmt/valid.eng

# Vocabulary Configuration
src_vocab: runs/vocab.src
tgt_vocab: runs/vocab.tgt
share_vocab: true

# Transforms 
transforms: [filtertoolong, onmt_tokenize, sentencepiece]

# Parameters 
src_seq_length: 100
tgt_seq_length: 100

# SentencePiece parameters
src_subword_model: ./data/sp/spm_shared.model
tgt_subword_model: ./data/sp/spm_shared.model

#Model Configuration
model_type: text
model_task: seq2seq
encoder_type: transformer
decoder_type: transformer
heads: 2
layers: 2
rnn_size: 128
transformer_ff: 256
dropout: 0.2
position_encoding: true
share_embeddings: true
param_init: 0.1
label_smoothing: 0.1

#Training Parameters
train_steps: 5000
valid_steps: 500
save_checkpoint_steps: 1000
early_stopping: 3
batch_type: tokens
batch_size: 512
valid_batch_size: 256
accum_count: [4]
optim: sgd
learning_rate: 0.5
decay_method: none              
max_grad_norm: 0.5
report_every: 100

#CPU Configuration
world_size: 1
gpu_ranks: []
train_on_gpu: false

# Saving / Logging 
save_model: runs/kap-en
tensorboard: true
tensorboard_log_dir: runs/tb
keep_checkpoint: 5
seed: 3435
#data config
save_data: runs/data_index

data:
  corpus_1:
    path_src: data/onmt/train.kap
    path_tgt: data/onmt/train.eng
  valid:
    path_src: data/onmt/valid.kap
    path_tgt: data/onmt/valid.eng

#vocab config
src_vocab: runs/vocab.src
tgt_vocab: runs/vocab.tgt
share_vocab: true

#Transformer
transforms: [filtertoolong, onmt_tokenize, sentencepiece]

# Transformer Parameters
src_seq_length: 256
tgt_seq_length: 256

# SentencePiece parameters
src_subword_model: ./data/sp/spm_shared.model
tgt_subword_model: ./data/sp/spm_shared.model

#Model config
model_type: text
model_task: seq2seq
encoder_type: transformer
decoder_type: transformer
heads: 8
layers: 6
rnn_size: 512
transformer_ff: 2048
dropout: 0.1
position_encoding: true
share_embeddings: true
param_init_glorot: true
label_smoothing: 0.1

# Training Parameters
train_steps: 50000
valid_steps: 1000
save_checkpoint_steps: 2000
early_stopping: 5
batch_type: tokens
batch_size: 4096               
valid_batch_size: 2048
accum_count: [2]
optim: adam
adam_beta1: 0.9
adam_beta2: 0.998
adam_eps: 1e-9
learning_rate: 2.0               
warmup_steps: 8000
decay_method: noam
max_grad_norm: 0.0              
report_every: 100

#GPU config
world_size: 1
gpu_ranks: [0]                    

#Saving n Logging
save_model: runs/kap-en-gpu
tensorboard: true
tensorboard_log_dir: runs/tb
keep_checkpoint: 10
seed: 3435

model_dtype: fp16               
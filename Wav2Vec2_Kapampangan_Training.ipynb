{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c2db497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import torchaudio\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC\n",
    ")\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.trainer import Trainer\n",
    "\n",
    "# Check for CUDA availability and print device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f2da24",
   "metadata": {},
   "source": [
    "# --- Configuration ---\n",
    "# IMPORTANT: Update these paths for Kapampangan training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cec59b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATED_DATA_FOLDER = 'data/Cleaned_Audio_Files'  # The folder with Kapampangan audio and transcriptions\n",
    "MODEL_OUTPUT_DIR = './kapampangan_wav2vec2_model'  # Directory to save the trained model\n",
    "BASE_MODEL = \"facebook/wav2vec2-large-xlsr-53\"  # Base model for fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252d84e",
   "metadata": {},
   "source": [
    "# --- 1. Load the Dataset ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebad40dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_custom_dataset(data_folder):\n",
    "    \"\"\"Loads the dataset from the metadata.csv file.\"\"\"\n",
    "    metadata_path = os.path.join(data_folder, \"metadata.csv\")\n",
    "    if not os.path.exists(metadata_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"metadata.csv not found in {data_folder}. \"\n",
    "        )\n",
    "    dataset_df = pd.read_csv(metadata_path)\n",
    "    # Convert DataFrame to Hugging Face Dataset object\n",
    "    custom_dataset = Dataset.from_pandas(dataset_df)\n",
    "    return custom_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3782449",
   "metadata": {},
   "source": [
    "# --- 2. Create Vocabulary for Kapampangan ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36d70721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(data):\n",
    "    \"\"\"\n",
    "    Extracts all unique characters from the Kapampangan transcription column\n",
    "    and creates a vocabulary file.\n",
    "    \"\"\"\n",
    "    # Regex to extract characters, handling potential variations\n",
    "    chars_to_ignore_regex = r\"[\\,\\?\\.\\!\\-\\;\\:\\\"'%\\[\\]]\"\n",
    "\n",
    "    def extract_all_chars(batch):\n",
    "        all_text = \" \".join(batch[\"transcription\"])\n",
    "        # Normalize and remove special characters\n",
    "        all_text = re.sub(chars_to_ignore_regex, '', all_text).lower()\n",
    "        # Create a set of unique characters\n",
    "        vocab = list(set(all_text))\n",
    "        return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "    # Extract vocabulary from the dataset\n",
    "    vocab_result = data.map(\n",
    "        extract_all_chars,\n",
    "        batched=True,\n",
    "        batch_size=-1,\n",
    "        keep_in_memory=True,\n",
    "        remove_columns=data.column_names\n",
    "    )\n",
    "\n",
    "    # Combine all unique characters from all batches\n",
    "    vocab_list = list(set(vocab_result[\"vocab\"][0]))\n",
    "    vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "\n",
    "    # Add special tokens for CTC loss\n",
    "    vocab_dict[\"|\"] = vocab_dict.pop(\" \") if \" \" in vocab_dict else len(vocab_dict)\n",
    "    vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "    vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "    \n",
    "    # Save the vocabulary as a json file\n",
    "    vocab_path = os.path.join(MODEL_OUTPUT_DIR, 'vocab.json')\n",
    "    if not os.path.exists(MODEL_OUTPUT_DIR):\n",
    "        os.makedirs(MODEL_OUTPUT_DIR)\n",
    "    with open(vocab_path, 'w') as vocab_file:\n",
    "        json.dump(vocab_dict, vocab_file)\n",
    "    \n",
    "    print(f\"Vocabulary created and saved to {vocab_path}\")\n",
    "    print(f\"Vocabulary size: {len(vocab_dict)}\")\n",
    "    return vocab_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6b5c05",
   "metadata": {},
   "source": [
    "# --- 3. Preprocess the Data ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96ca01d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, processor):\n",
    "    \"\"\"\n",
    "    Prepares the dataset for training:\n",
    "    1. Loads and resamples audio.\n",
    "    2. Tokenizes Kapampangan transcriptions.\n",
    "    \"\"\"\n",
    "    import librosa\n",
    "    import soundfile as sf\n",
    "\n",
    "    total_before = len(dataset)\n",
    "    \n",
    "    def prepare_dataset(batch):\n",
    "        try:\n",
    "            audio_path = batch[\"file_path\"]\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "            if sr != 16000:\n",
    "                resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "                waveform = resampler(waveform)\n",
    "\n",
    "            # Convert waveform tensor to numpy array and flatten\n",
    "            audio_array = waveform.squeeze().numpy()\n",
    "\n",
    "            batch[\"input_values\"] = processor(audio_array, sampling_rate=16000).input_values[0]\n",
    "            batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "            with processor.as_target_processor():\n",
    "                batch[\"labels\"] = processor(batch[\"transcription\"]).input_ids\n",
    "            return batch\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Failed to Process\")\n",
    "            print(f\"File: {batch.get('file_path', 'Path not found')}\")\n",
    "            print(f\"Error: {repr(e)}\")  # This will give the actual exception message\n",
    "            print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            return None\n",
    "\n",
    "    # Process each example individually\n",
    "    processed_examples = []\n",
    "    for i in range(len(dataset)):\n",
    "        example = dataset[i]\n",
    "        processed_example = prepare_dataset(example)\n",
    "        if processed_example is not None:\n",
    "            processed_examples.append(processed_example)\n",
    "    \n",
    "    # Create a new dataset from processed examples\n",
    "    from datasets import Dataset\n",
    "    dataset = Dataset.from_list(processed_examples)\n",
    "    total_after = len(dataset)\n",
    "    print(f\"Preprocessing complete: {total_after} / {total_before} samples successfully processed.\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46646bb9",
   "metadata": {},
   "source": [
    "# --- 4. Define Metrics and Data Collator ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b807b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that dynamically pads the inputs and labels for CTC.\n",
    "    \"\"\"\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "        self.padding = \"longest\"\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def compute_metrics(pred, processor, wer_metric):\n",
    "    \"\"\"Computes Word Error Rate (WER) for evaluation.\"\"\"\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = torch.argmax(torch.from_numpy(pred_logits), dim=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids)\n",
    "    label_str = processor.tokenizer.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91789aa",
   "metadata": {},
   "source": [
    "# --- Main Training Execution ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fce35d",
   "metadata": {},
   "source": [
    "# Step 1: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99a0aede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Kapampangan Dataset ---\n",
      "Dataset split into 160 training samples and 40 evaluation samples.\n"
     ]
    }
   ],
   "source": [
    "# Load custom dataset\n",
    "print(\"--- Step 1: Loading Kapampangan Dataset ---\")\n",
    "raw_dataset = load_custom_dataset(VALIDATED_DATA_FOLDER)\n",
    "\n",
    "from datasets import DatasetDict\n",
    "RANDOM_SEED = 42\n",
    "SPLIT_RATIO = 0.2  # 20% for evaluation\n",
    "\n",
    "if len(raw_dataset) > 1:\n",
    "    dataset_split = raw_dataset.train_test_split(\n",
    "        test_size=SPLIT_RATIO,\n",
    "        shuffle=True,\n",
    "        seed=RANDOM_SEED\n",
    "    )\n",
    "    train_dataset = dataset_split['train']\n",
    "    eval_dataset = dataset_split['test']\n",
    "    print(f\"Dataset split into {len(train_dataset)} training samples and {len(eval_dataset)} evaluation samples.\")\n",
    "else:\n",
    "    train_dataset = raw_dataset\n",
    "    eval_dataset = raw_dataset\n",
    "    print(\"Warning: Dataset is too small for a split. Evaluating on the training set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b6cf3",
   "metadata": {},
   "source": [
    "# Step 2: Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50ff88b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Creating Kapampangan Vocabulary ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f796b93ab29c43ba84307ff6b3876b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary created and saved to ./kapampangan_wav2vec2_model\\vocab.json\n",
      "Vocabulary size: 26\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 2: Creating Kapampangan Vocabulary ---\")\n",
    "vocab_path = create_vocabulary(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f8600",
   "metadata": {},
   "source": [
    "# Step 3: Setup Processor (Tokenizer + Feature Extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5957d263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Setting up Processor ---\n",
      "Processor created and saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 3: Setting up Processor ---\")\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\n",
    "    vocab_file=vocab_path,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    word_delimiter_token=\"|\"\n",
    ")\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size=1,\n",
    "    sampling_rate=16000,\n",
    "    padding_value=0.0,\n",
    "    do_normalize=True,\n",
    "    return_attention_mask=False\n",
    ")\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "# Save for reuse\n",
    "processor.save_pretrained(MODEL_OUTPUT_DIR)\n",
    "print(\"Processor created and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1908391",
   "metadata": {},
   "source": [
    "# Step 4: Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7660940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Preprocessing Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:170: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete: 160 / 160 samples successfully processed.\n",
      "Preprocessing complete: 40 / 40 samples successfully processed.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 4: Preprocessing Data ---\")\n",
    "processed_train_dataset = preprocess_data(train_dataset, processor)\n",
    "processed_eval_dataset = preprocess_data(eval_dataset, processor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f83356d",
   "metadata": {},
   "source": [
    "# Step 5: Setup Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b7580",
   "metadata": {},
   "source": [
    "# Step 6: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ceff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 6: Starting Kapampangan Training ---\n",
      "This may take some time depending on your machine and dataset size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 94/600 51:08 < 4:41:19, 0.03 it/s, Epoch 4.65/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>15.471600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>15.111500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>10.815000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:170: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:170: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:170: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:170: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 6: Starting Kapampangan Training ---\")\n",
    "print(\"This may take some time depending on your machine and dataset size.\")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8f3db",
   "metadata": {},
   "source": [
    "# Step 7: Save the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80062d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 7: Saving Final Model ---\")\n",
    "trainer.save_model(MODEL_OUTPUT_DIR)\n",
    "print(f\"Training complete! Model saved in: {MODEL_OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd6d3e",
   "metadata": {},
   "source": [
    "# Step 8: Evaluate on Evaluation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8749dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 8: Evaluating on Validation Set ---\")\n",
    "\n",
    "# Optionally reload model\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(MODEL_OUTPUT_DIR).to(device)\n",
    "\n",
    "predictions = trainer.predict(processed_eval_dataset)\n",
    "metrics = compute_metrics_wrapper(predictions)\n",
    "\n",
    "print(f\"Validation WER: {metrics['wer']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f8fc7",
   "metadata": {},
   "source": [
    "# Show Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb601570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "print(\"\\n--- Sample Predictions ---\")\n",
    "pred_ids = torch.argmax(torch.from_numpy(predictions.predictions), dim=-1)\n",
    "decoded_preds = processor.batch_decode(pred_ids)\n",
    "decoded_labels = processor.batch_decode(predictions.label_ids, group_tokens=False)\n",
    "\n",
    "for i in sample(range(len(decoded_preds)), 5):\n",
    "    print(f\"[{i+1}]\")\n",
    "    print(f\"Kapampangan (Predicted) : {decoded_preds[i]}\")\n",
    "    print(f\"Kapampangan (Reference) : {decoded_labels[i]}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c2db497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import torchaudio\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC\n",
    ")\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.trainer import Trainer\n",
    "\n",
    "# Check for CUDA availability and print device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f2da24",
   "metadata": {},
   "source": [
    "# --- Configuration ---\n",
    "# IMPORTANT: Update these paths for Kapampangan training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cec59b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATED_DATA_FOLDER = 'data/Cleaned_Audio_Files'  # The folder with Kapampangan audio and transcriptions\n",
    "MODEL_OUTPUT_DIR = './kapampangan_wav2vec2_model'  # Directory to save the trained model\n",
    "BASE_MODEL = \"facebook/wav2vec2-large-xlsr-53\"  # Base model for fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252d84e",
   "metadata": {},
   "source": [
    "# --- 1. Load the Dataset ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebad40dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_custom_dataset(data_folder):\n",
    "    \"\"\"Loads the dataset from the metadata.csv file.\"\"\"\n",
    "    metadata_path = os.path.join(data_folder, \"metadata.csv\")\n",
    "    if not os.path.exists(metadata_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"metadata.csv not found in {data_folder}. \"\n",
    "            \"Please ensure you have run the prepare_kapampangan_dataset.py script first.\"\n",
    "        )\n",
    "    dataset_df = pd.read_csv(metadata_path)\n",
    "    # Convert DataFrame to Hugging Face Dataset object\n",
    "    custom_dataset = Dataset.from_pandas(dataset_df)\n",
    "    return custom_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3782449",
   "metadata": {},
   "source": [
    "# --- 2. Create Vocabulary for Kapampangan ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36d70721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(data):\n",
    "    \"\"\"\n",
    "    Extracts all unique characters from the Kapampangan transcription column\n",
    "    and creates a vocabulary file.\n",
    "    \"\"\"\n",
    "    # Regex to ignore punctuation/symbols\n",
    "    chars_to_ignore_regex = r\"[\\,\\?\\.\\!\\-\\;\\:\\â€œ\\â€\\â€˜\\â€™\\'\\\"\\%\\[\\]\\(\\)\\â€¦]\"\n",
    "\n",
    "    def extract_all_chars(batch):\n",
    "        all_text = \" \".join(batch[\"transcription\"])\n",
    "        # Normalize and clean\n",
    "        all_text = re.sub(chars_to_ignore_regex, '', all_text).lower()\n",
    "        return {\"all_text\": [all_text]}\n",
    "\n",
    "    # Concatenate all transcriptions into a single string\n",
    "    vocab_result = data.map(\n",
    "        extract_all_chars,\n",
    "        batched=True,\n",
    "        batch_size=-1,\n",
    "        keep_in_memory=True,\n",
    "        remove_columns=data.column_names\n",
    "    )\n",
    "\n",
    "    all_text = \" \".join(vocab_result[\"all_text\"])\n",
    "    # Sorted ensures reproducibility\n",
    "    vocab_list = sorted(list(set(all_text)))\n",
    "\n",
    "    vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "\n",
    "    # Add special tokens for CTC\n",
    "    if \" \" in vocab_dict:\n",
    "        vocab_dict[\"|\"] = vocab_dict.pop(\" \")\n",
    "    else:\n",
    "        vocab_dict[\"|\"] = len(vocab_dict)\n",
    "\n",
    "    vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "    vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "\n",
    "    # Save the vocabulary\n",
    "    vocab_path = os.path.join(MODEL_OUTPUT_DIR, 'vocab.json')\n",
    "    os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "    with open(vocab_path, 'w', encoding='utf-8') as vocab_file:\n",
    "        json.dump(vocab_dict, vocab_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… Vocabulary created and saved to {vocab_path}\")\n",
    "    print(f\"ðŸ“Š Vocabulary size: {len(vocab_dict)}\")\n",
    "    return vocab_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6b5c05",
   "metadata": {},
   "source": [
    "# --- 3. Preprocess the Data ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96ca01d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, processor):\n",
    "    \"\"\"\n",
    "    Prepares the dataset for training:\n",
    "    1. Loads and resamples audio.\n",
    "    2. Tokenizes Kapampangan transcriptions.\n",
    "    \"\"\"\n",
    "    import librosa\n",
    "    import soundfile as sf\n",
    "\n",
    "    total_before = len(dataset)\n",
    "    \n",
    "    def prepare_dataset(batch):\n",
    "        try:\n",
    "            audio_path = batch[\"file_path\"]\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "            if sr != 16000:\n",
    "                resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "                waveform = resampler(waveform)\n",
    "\n",
    "            # Convert waveform tensor to numpy array and flatten\n",
    "            audio_array = waveform.squeeze().numpy()\n",
    "\n",
    "            batch[\"input_values\"] = processor(audio_array, sampling_rate=16000).input_values[0]\n",
    "            batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "            with processor.as_target_processor():\n",
    "                batch[\"labels\"] = processor(batch[\"transcription\"]).input_ids\n",
    "            return batch\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Failed to Process\")\n",
    "            print(f\"File: {batch.get('file_path', 'Path not found')}\")\n",
    "            print(f\"Error: {repr(e)}\")  # This will give the actual exception message\n",
    "            print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            return None\n",
    "\n",
    "    # Process each example individually\n",
    "    processed_examples = []\n",
    "    for i in range(len(dataset)):\n",
    "        example = dataset[i]\n",
    "        processed_example = prepare_dataset(example)\n",
    "        if processed_example is not None:\n",
    "            processed_examples.append(processed_example)\n",
    "    \n",
    "    # Create a new dataset from processed examples\n",
    "    from datasets import Dataset\n",
    "    dataset = Dataset.from_list(processed_examples)\n",
    "    total_after = len(dataset)\n",
    "    print(f\"Preprocessing complete: {total_after} / {total_before} samples successfully processed.\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46646bb9",
   "metadata": {},
   "source": [
    "# --- 4. Define Metrics and Data Collator ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b807b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that dynamically pads the inputs and labels for CTC.\n",
    "    \"\"\"\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "        self.padding = \"longest\"\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def compute_metrics(pred, processor, wer_metric):\n",
    "    \"\"\"Computes Word Error Rate (WER) for evaluation.\"\"\"\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = torch.argmax(torch.from_numpy(pred_logits), dim=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids)\n",
    "    label_str = processor.tokenizer.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91789aa",
   "metadata": {},
   "source": [
    "# --- Main Training Execution ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fce35d",
   "metadata": {},
   "source": [
    "# Step 1: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99a0aede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Kapampangan Dataset ---\n",
      "Dataset split into 1990 training samples and 498 evaluation samples.\n"
     ]
    }
   ],
   "source": [
    "# Load custom dataset\n",
    "print(\"--- Step 1: Loading Kapampangan Dataset ---\")\n",
    "raw_dataset = load_custom_dataset(VALIDATED_DATA_FOLDER)\n",
    "\n",
    "from datasets import DatasetDict\n",
    "RANDOM_SEED = 42\n",
    "SPLIT_RATIO = 0.2  # 20% for evaluation\n",
    "\n",
    "if len(raw_dataset) > 1:\n",
    "    dataset_split = raw_dataset.train_test_split(\n",
    "        test_size=SPLIT_RATIO,\n",
    "        shuffle=True,\n",
    "        seed=RANDOM_SEED\n",
    "    )\n",
    "    train_dataset = dataset_split['train']\n",
    "    eval_dataset = dataset_split['test']\n",
    "    print(f\"Dataset split into {len(train_dataset)} training samples and {len(eval_dataset)} evaluation samples.\")\n",
    "else:\n",
    "    train_dataset = raw_dataset\n",
    "    eval_dataset = raw_dataset\n",
    "    print(\"Warning: Dataset is too small for a split. Evaluating on the training set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b6cf3",
   "metadata": {},
   "source": [
    "# Step 2: Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50ff88b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Creating Kapampangan Vocabulary ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1990/1990 [00:00<00:00, 248531.00 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vocabulary created and saved to ./kapampangan_wav2vec2_model\\vocab.json\n",
      "ðŸ“Š Vocabulary size: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 2: Creating Kapampangan Vocabulary ---\")\n",
    "vocab_path = create_vocabulary(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f8600",
   "metadata": {},
   "source": [
    "# Step 3: Setup Processor (Tokenizer + Feature Extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5957d263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Setting up Processor ---\n",
      "Processor created and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 3: Setting up Processor ---\")\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\n",
    "    vocab_file=vocab_path,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    word_delimiter_token=\"|\"\n",
    ")\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size=1,\n",
    "    sampling_rate=16000,\n",
    "    padding_value=0.0,\n",
    "    do_normalize=True,\n",
    "    return_attention_mask=False\n",
    ")\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "# Save for reuse\n",
    "processor.save_pretrained(MODEL_OUTPUT_DIR)\n",
    "print(\"Processor created and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1908391",
   "metadata": {},
   "source": [
    "# Step 4: Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7660940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Preprocessing Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete: 1990 / 1990 samples successfully processed.\n",
      "Preprocessing complete: 498 / 498 samples successfully processed.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 4: Preprocessing Data ---\")\n",
    "processed_train_dataset = preprocess_data(train_dataset, processor)\n",
    "processed_eval_dataset = preprocess_data(eval_dataset, processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16a7aa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f83356d",
   "metadata": {},
   "source": [
    "# Step 5: Setup Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "616c063f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 5: Setting up Model and Trainer ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     52\u001b[39m     wer = wer_metric.compute(predictions=pred_str, references=label_str)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mwer\u001b[39m\u001b[33m\"\u001b[39m: wer}\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m trainer = \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessed_train_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessed_eval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\trainer.py:404\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;28mself\u001b[39m.args = args\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m enable_full_determinism(\u001b[38;5;28mself\u001b[39m.args.seed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.full_determinism \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[38;5;28mself\u001b[39m.hp_name = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[38;5;28mself\u001b[39m.deepspeed = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\trainer_utils.py:104\u001b[39m, in \u001b[36mset_seed\u001b[39m\u001b[34m(seed, deterministic)\u001b[39m\n\u001b[32m    102\u001b[39m np.random.seed(seed)\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m     torch.cuda.manual_seed_all(seed)\n\u001b[32m    106\u001b[39m     \u001b[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\_compile.py:32\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     30\u001b[39m     fn.__dynamo_disable = disable_fn\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(\n\u001b[32m    742\u001b[39m     _is_skip_guard_eval_unsafe_stance()\n\u001b[32m    743\u001b[39m )\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    747\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\random.py:46\u001b[39m, in \u001b[36mmanual_seed\u001b[39m\u001b[34m(seed)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcuda\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.cuda._is_in_bad_fork():\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmps\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.mps._is_in_bad_fork():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\cuda\\random.py:127\u001b[39m, in \u001b[36mmanual_seed_all\u001b[39m\u001b[34m(seed)\u001b[39m\n\u001b[32m    124\u001b[39m         default_generator = torch.cuda.default_generators[i]\n\u001b[32m    125\u001b[39m         default_generator.manual_seed(seed)\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:249\u001b[39m, in \u001b[36m_lazy_call\u001b[39m\u001b[34m(callable, **kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, **kwargs):\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    251\u001b[39m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[32m    252\u001b[39m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[32m    253\u001b[39m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[32m    254\u001b[39m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\cuda\\random.py:125\u001b[39m, in \u001b[36mmanual_seed_all.<locals>.cb\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[32m    124\u001b[39m     default_generator = torch.cuda.default_generators[i]\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[43mdefault_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 5: Setting up Model and Trainer ---\")\n",
    "\n",
    "# Collator and metric\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor)\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "# Load base model\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    vocab_size=len(tokenizer)\n",
    ")\n",
    "\n",
    "# Freeze feature extractor first (can unfreeze later if needed)\n",
    "model.freeze_feature_encoder()\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2_kapampangan\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=30,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_ids = np.where(pred.label_ids != -100, pred.label_ids, tokenizer.pad_token_id)\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_train_dataset,   \n",
    "    eval_dataset=processed_eval_dataset,    \n",
    "    tokenizer=processor,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f256fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch ver: 2.6.0+cu124\n",
      "Cuda avail: True\n",
      "Device: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Pytorch ver: {torch.__version__}\")\n",
    "print(f\"Cuda avail: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b7580",
   "metadata": {},
   "source": [
    "# Step 6: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ceff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 6: Starting Kapampangan Training ---\n",
      "This may take some time depending on your machine and dataset size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/3720 [01:09<19:20:24, 18.74s/it]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Step 6: Starting Kapampangan Training ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThis may take some time depending on your machine and dataset size.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\trainer.py:1938\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1937\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2279\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2276\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_begin(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2278\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.accumulate(model):\n\u001b[32m-> \u001b[39m\u001b[32m2279\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2282\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2283\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2284\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2285\u001b[39m ):\n\u001b[32m   2286\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2287\u001b[39m     tr_loss += tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3318\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs)\u001b[39m\n\u001b[32m   3315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3317\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3318\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3320\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3322\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3323\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3324\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3363\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs)\u001b[39m\n\u001b[32m   3361\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3362\u001b[39m     labels = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3363\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3364\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3365\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:2228\u001b[39m, in \u001b[36mWav2Vec2ForCTC.forward\u001b[39m\u001b[34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[39m\n\u001b[32m   2225\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m labels.max() >= \u001b[38;5;28mself\u001b[39m.config.vocab_size:\n\u001b[32m   2226\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLabel values must be <= vocab_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.vocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2228\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwav2vec2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2229\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2231\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2232\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2234\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2236\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   2237\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1809\u001b[39m, in \u001b[36mWav2Vec2Model.forward\u001b[39m\u001b[34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1804\u001b[39m output_hidden_states = (\n\u001b[32m   1805\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m   1806\u001b[39m )\n\u001b[32m   1807\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1809\u001b[39m extract_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1810\u001b[39m extract_features = extract_features.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m   1812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1813\u001b[39m     \u001b[38;5;66;03m# compute reduced attention_mask corresponding to feature vectors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:463\u001b[39m, in \u001b[36mWav2Vec2FeatureEncoder.forward\u001b[39m\u001b[34m(self, input_values)\u001b[39m\n\u001b[32m    458\u001b[39m         hidden_states = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    459\u001b[39m             conv_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m    460\u001b[39m             hidden_states,\n\u001b[32m    461\u001b[39m         )\n\u001b[32m    462\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m         hidden_states = \u001b[43mconv_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:335\u001b[39m, in \u001b[36mWav2Vec2LayerNormConvLayer.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    332\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.conv(hidden_states)\n\u001b[32m    334\u001b[39m hidden_states = hidden_states.transpose(-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m hidden_states = hidden_states.transpose(-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m    338\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.activation(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217\u001b[39m, in \u001b[36mLayerNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2910\u001b[39m, in \u001b[36mlayer_norm\u001b[39m\u001b[34m(input, normalized_shape, weight, bias, eps)\u001b[39m\n\u001b[32m   2900\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[32m   2901\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m   2902\u001b[39m         layer_norm,\n\u001b[32m   2903\u001b[39m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2908\u001b[39m         eps=eps,\n\u001b[32m   2909\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2910\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2911\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\n\u001b[32m   2912\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 6: Starting Kapampangan Training ---\")\n",
    "print(\"This may take some time depending on your machine and dataset size.\")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e071d5f9",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e54831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAZV9JREFUeJzt3Qd4VNXWBuCVXoAk9BAIofeOiiACSkel2JEr2PC3XrxYwULz2kW9gl3EAmKjKCKCSJHeew0EQkkhQBJCejL/861whkklIVMz3/s8x2TOnDlzZs/IrKy99t4eJpPJJERERERuxNPRF0BERERkbwyAiIiIyO0wACIiIiK3wwCIiIiI3A4DICIiInI7DICIiIjI7TAAIiIiIrfDAIiIiIjcDgMgIiIicjsMgIioXO677z5p0KDBFT124sSJ4uHhYfVrIiK6HAZARBUUAovSbCtWrBB3Ddws2yEoKEjat28v7777rmRkZDj68ojIxjy4FhhRxfTdd9/lu/3NN9/I0qVL5dtvv823v2/fvlK7du0rfp6srCzJzc0VPz+/Mj82OztbN39/f3FEADRnzhz54osv9HZiYqL88ssvGhDeddddeh8RVVwMgIjcxBNPPCHTp0+Xy/0vn5qaKoGBgVLRIQD6+eefJSUlxbwPgVyXLl1k8+bNcvLkSQkLCyv0OLRfenq6BAQE2OU63eX9ILI3doERubFevXpJmzZtZMuWLdKjRw/9oh0/frzet2DBArnppps0CEB2p3HjxjJlyhTJyckpsQbo6NGj2qX0zjvvyGeffaaPw+Ovvvpq2bRp02VrgHAbwdr8+fP12vDY1q1by+LFiwtdP7I1V111lWaQ8DyffvppueqKPD09tU2M1wF4bTfffLP8+eef+lwIfPA8cOTIEbnjjjukWrVq2nbXXnut/P7774XOe+zYMRk8eLBUqlRJatWqJf/5z3/0fAW7IEt6P9AtN2HCBGnSpIm2SXh4uDz33HOFuuuQ5evevbuEhIRI5cqVpXnz5uZzGD788ENtU5y/atWq+rpmz559RW1G5Kq8HX0BRORYZ86ckYEDB8rdd98t//rXv8zdYTNnztQv0LFjx+rPv//+W1555RVJTk6Wt99++7LnxRfq+fPn5f/+7//0i/6tt96SW2+9VYMGHx+fEh+7evVqmTt3rjz22GNSpUoV+d///ie33XabREdHS/Xq1fWYbdu2yYABA6ROnToyadIkDcwmT54sNWvWLFd7HD58WH8azwMHDhyQ4cOH62sZPXq0BhVxcXHSrVs3zdD8+9//1uO//vprDXSQWRo2bJg+9sKFC3LjjTdKTEyMjBkzRkJDQ7Vtli9fXur3A5kpnBft8vDDD0vLli1l165d8t5778nBgwc1WIQ9e/ZosNauXTttCwRKkZGRsmbNGvP5P//8c73e22+/Xa8H2aydO3fKhg0b5J577ilX2xG5FHSBEVHF9/jjj6PvK9++nj176r5PPvmk0PGpqamF9v3f//2fKTAw0JSenm7eN2rUKFNERIT5dlRUlJ6zevXqprNnz5r3L1iwQPf/9ttv5n0TJkwodE247evra4qMjDTv27Fjh+7/8MMPzftuueUWvZaTJ0+a9x06dMjk7e1d6JxFwXVXqlTJdPr0ad3wfK+99prJw8PD1K5dO/NxeG043+LFi/M9/qmnntL9//zzj3nf+fPnTQ0bNjQ1aNDAlJOTo/veffddPW7+/Pnm49LS0kwtWrTQ/cuXL7/s+/Htt9+aPD098z0X4Dgcv2bNGr393nvv6W28nuIMGTLE1Lp168u2D1FFxy4wIjeHLMH9999faL9ljQsyOQkJCXL99ddrxmP//v2XPS8KidG9YsBjARmgy+nTp492aRmQ0cAoLeOxyPb89ddfMnTo0Hx1OugeQvaktJCdQcYIGx6LrqKuXbvKvHnz8h3XsGFD6d+/f759ixYtkmuuuUa7mwzIlCFDg+6zvXv36j503dWtW1czOAZ02SGTVNr346efftKsT4sWLfR9MDZklsDIJqHby+i+RNaoKDjmxIkThbojidwNAyAiN4cvZ19f30L70Z2Cbpzg4GANPhAkoEsGkpKSLnve+vXr57ttBEPnzp0r82ONxxuPjY+Pl7S0NA1aCipqX3EQiKBmBtuqVavk+PHj2l3UqFGjQgFQUXU96AorCIGKcb/xE8Fcwbqk4q6zqPfj0KFD+n4YwZqxNWvWzNweRtB53XXXyUMPPaRdZ+hG+/HHH/MFQ88//7wGagjemjZtKo8//ni+LjIid8EaICI3V9RoJgwJ79mzpwY+qCXBFziCha1bt+oXaHHZBUteXl5F7i/NwNPyPLYs8DzINl2OvUZ8FfdcaO+2bdvK1KlTi3wMCqKNxyKQQ0YIxdjIPv3www+aKVqyZIm+XgRoqGlauHCh3o+h/x999JHWd6GWishdMAAiokIwMgnFuChExmgkQ1RUlDgDjKRCQIYC34KK2mcLERERGkgUZHQP4n7jJ7rDELxZZoHKcp0IQHfs2CG9e/e+7Ag3jGTDcdgQML322mvy4osvalBkBHsYjYZsEbbMzEwtTv/vf/8r48aNc8icTESOwC4wIio2A2OZccEXJTIFzsDI3GD006lTp/IFFX/88YddrmHQoEGyceNGWbduXb6aIgz9x9D5Vq1a6T7UDmFOoV9//dV8HEZeYTRWad155516jqIeg65APC+cPXu20P0dOnTQn8ZweQS2ltDdhmvFe41JLYncBTNARFQIhnej5mbUqFE6ZBpZB8wg7UzzpmK+H3TroObl0Ucf1cLoadOm6Tw627dvt/nzv/DCC/L9999r0TXaCHMBYRg8smToVkImBjB0HteFYfQYdo5h+7NmzTJnWkozZ9G9996rtTyPPPKIZnLwmvF6kW3CfmOOInRXogsM8zch84TaIASt9erVMxdr9+vXT4fi4xyoE9q3b59eHx6DKQeI3AUDICIqBHPaoEbk6aeflpdeekmDIRRAo1ul4GgoR+ncubNme5555hl5+eWXtQ4GAQC+0EszSq28EDysXbtWa6IwsSCyOhit9ttvv2kwYTDmUHryySflgw8+0NsjR47UIBNzG5WmywnBFLJdmPcHS5pglBomMUSxNoIqoxgaI80wAm3GjBk6SqxGjRpay4XaHhSzGwEZAjB0j2EWbARHCODwPhO5Ey6FQUQVCobGY8QURk45s/fff19nhMaQdIz8IiL7Yg0QEbks1L9YQtCD+XmM5Syc9TqRLcJyGhiGzuCHyDHYBUZELgtdQFiLDD8x387HH3+sRb1YI8uZYJQV5jZCQTLmUPruu++0mw5dUUTkGAyAiMhlYS0wFCLHxsbqDMqYxRnDvpFZcSaom/riiy804EHxMkZdzZkzR4ehE5FjsAaIiIiI3A5rgIiIiMjtMAAiIiIit8MaoCJg3R3MLotJwUozSRkRERE5Hqp6zp8/L2FhYebJSIvDAKgICH6MxQWJiIjItRw/flwn+SwJA6AiGNPBowGxGnZ5YX0dTNmPKeh9fHyscIVUEra3/bCt7YvtbV9sb9dr6+TkZE1glGZZFwZARTC6vRD8WCsAwrT1OBf/J7I9trf9sK3ti+1tX2xv123r0pSvsAiaiIiI3A4DICIiInI7DICIiIjI7Ti0BmjVqlXy9ttvy5YtWyQmJkbmzZunKzlfrg/vrbfekmeffbbI+yZOnCiTJk3Kt6958+a67g4RETk3LBWCehBngOvw9vbWxWtxXeT4tkZ9kJeXl+sHQBcuXJD27dvLAw88oIsFFoSgyNIff/whDz74oNx2220lnrd169by119/mW+jUYmIyLnnb8GabomJieJM1xQaGqojgjknnPO0dUhIiB5b3vfEoZHBwIEDdSsOXqClBQsWyA033KArP5cEAU/BxxIRkfMygp9atWrpaCBnCDgwKW5KSopUrlz5spPqke3bGkFSamqqxMfH6+06deqU6zldJjUSFxcnv//+u3z99deXPfbQoUM6C6S/v7+uDv36669L/fr17XKdRERUNujyMIKf6tWrizN9KWdmZup3CQMg52jrgIAA/YkgCJ+X8nSHuUwAhMAHExsV1VVmqUuXLjJz5kyt+0EXGuqBrr/+etm9e3exEyNlZGToZjmRktEnaY2+aOMcztKvXdGxve2HbW1fFbW98e8v/rrHlx++CJ0Frsn46UzXVRGZytDW+JzguLS0NPHz88t3X1n+3/AwGc/qYEh3FiyCttSiRQvp27evfPjhh2U6L/6qiIiIkKlTp2r9UGkLp2H27NmaiiUiItsxyhYwg6+vr6+jL4ecHDJFqBVCt2l2dna++9BFds8990hSUtJlJzJ2iQzQP//8IwcOHJAffvihzI9FsVSzZs0kMjKy2GPGjRsnY8eOLTSVNqbkttZM0EuXLtUAjrOJ2h7b237Y1vZVUdsbI3/whYb6D/x172wLa3JhbOdqa3xe0BXWo0ePQp8XowenNFwiAPryyy+lc+fOOmKsrFBUdfjwYbn33nuLPQYptIJpNMA/MNb8R8ba56OSsb3th21tXxWtvVEDhC891H44U62N0RVjXJu9NGjQQJ566indSmPFihU6QOjcuXP6R78ryi1DW+N+HFfU/wdl+f/CoZ80BCfbt2/XDaKiovT36OjofNHcTz/9JA899FCR5+jdu7dMmzbNfPuZZ56RlStXytGjR2Xt2rUybNgwLZIaPny4HV4RERG5C3wJl7ShvOJKbNq0SR5++OFSH9+tWzeteQ0ODhZbWrFihb4uZ5qqoDwcmgHavHmzRq0Goxtq1KhRWsgMc+bM0dRYcQEMsjsJCQnm2ydOnNBjz5w5IzVr1pTu3bvL+vXr9XdnhNeWlWMSX2/n+auHiIguz3KuOpRovPLKK1quYUCXnuW/9ch0lWZeurJ+X6FuilO/lJ1Dv3V79eqlH4qCmxH8AKJgFDUVF9ki02MZZSNgOnXqlI4qQDCE240bNxZnNe3vSHnmpx1yISN/IRcRETk3BB3Ghu8oZEeM21h9APUsmMAXJRwos1i9erX+0T5kyBCpXbu2BkhXX311vol7jS6w999/33wb5/3iiy+0RwMDc5o2bSq//vprsZkZfIeiK+zPP/+Uli1b6vMMGDAgX8CWnZ0t//73v/U4TD3w/PPPa/KhuIFIpYEuuJEjR0rVqlX1OjHPH6alMRw7dkxuueUWvb9SpUo6afGiRYvMjx09erS2C+p78Bq/+uorsSWmHRzsUHyKBj+xyemOvhQiIqeBP4bTs3IcsllzcPQLL7wgb7zxhuzbt0/atWunpR+DBg2SZcuWybZt2zQwQVBgWfpRFIxUvvPOO2Xnzp36+BEjRsjZs2eLPR6Jg3feeUe+/fZbXXYK50eJiOHNN9+UWbNmaZCxZs0aLTeZP39+uV7rfffdpz07CM7WrVun7YhrNYamP/7445qcwPXs2rVLr8HIkhnZM8z3h7b6+OOPpUaNGmJLLlEEXZHhfzbIzOYcE0REhozsXHl81laHPPf0EZ3E38c6601NnjxZR+0ZqlWrlm9Az5QpU3QKGAQNTzzxRInBhVEK8tprr8n//vc/2bhxowZQRUHQ8cknn5h7QHBuXIvhww8/1BHQyCoBammNbMyVQKYHrwHBFGqSAAEWRlQjsLrjjjs0CMNSVm3bttX7LVd1wH0IEK+66iotckYWzNaYAXKgrJxcycnN+0uDARARUcWDL3RLyAAhE4OuKXQ/IQOCjMflMkAIDgzoPsIULcaSEEVBF5Rl+QeWjTCOxxw5cXFxcs0115jvx2AhdNVdKbwG1DdhMmIDutYwKTHuA3S5vfrqq3LdddfJhAkTNJtleOSRR2Tu3LnSqVMnee6553QQk60xA+RAaRezP5CZwwCIiMjg5+2pmRhHPbe1IFixhOAHczmhe6pJkyZa73L77bfr5H4lKTi8GzU/Jc2YXNTxjp73+KGHHpL+/ftrN9eSJUt0map3331XnnzySa0XQkCEef/QPYgR3ugyQzvZCjNATtD9BcwAERHl/8JGN5QjNltOeoguInRnoesJXUEomMZgHntCwXbt2rV1uL0BI9S2br3yLkdktFBYvWHDBvM+jMZGXU+rVq3M+9AlZmR7nn76afn888/N96HmB4XY3333nRaBf/bZZ2JLzAA5UEbWpaCHARARUcWH0U348kfhMwKtl19+2SHrjD355JOagUEWCktNoSYII7FKE/yhgNlybU08BnVNGN2GkVyffvqp3o8C8Lp16+p+wMSOyPRgdQY81/LlyzVwAnSJ4Xd0GaJ+aeHCheb7bIUBkJNkgFDwR0REFRvWpXzggQe0UBgZDww/L8vyDdby/PPP61paGLaO+h9MOYPuqdKsro4lKCzhMcj+YETZmDFj5Oabb9YuPRyHwmqjOw5ZJnRrYYoa1DChgPu9994zz2WEIm3UQqFbEIuYYxobW3KaxVCdCT6MSBGWZjG10kA0iw8BhgNa9svuOpEk7/91UH8f3CFMhnSoW+7nouLbm6yPbW1fFbW9sbYTVgJo2LChU60FhswMvg/wPeBMS3TY6rW2bNlSh9pjZJozt3VJn5eyfH8zA+RA6dmsASIiIvs7duyYFiL37NlT5+bBMHgEFVhJ3V1U7JDWlYqgOQqMiIjsxNPTU2eMxkzUGJaOuh7MSG3ruhtnwgyQA6VlMgNERET2Fx4eriPS3BkzQA6UbhH0MAAiIiKyHwZADpRukQHiKDAiIiL7YQDkQCyCJiIicgwGQA7EImgiIiLHYADkQOmcCZqIiMghGAA5zUzQl34nIiIi22IA5CQZIBZBExFRUbBYKtbb2r59u6MvpUJhAGRHJ86lyYoD8bIvJm/dFxZBExG5NqzsjuCk4IZ1ruypV69eutgolR4nQrSj7ccTZeGuOLm+aQ1pWScofxE0AyAiIpeEYAcLgVry8/Nz2PVQ6TADZEfBAXmLFyanZ+tPywAoJ9ekGxERuRYEO6Ghofm2qlWr6n1YW+uuu+4qtKgtVoL/5ptv9PbixYule/fuEhISItWrV9fV1A8fPmzVa/zll1+kdevWeq0NGjSQd999N9/9H330kTRt2lQXF61du7bcfvvt5vt+/vlnadu2ra7Sjuvr06ePXLhwQVwdM0B2VMU/r7mT0rLEZDJJhkUNkJEFCvD1ctDVERE5EZNJJDvDMc/t7Sfi4WGVU40YMULuuOMOSUlJkcqVK+u+P//8U1JTU2XYsGF6G8HE2LFjpV27dnrcK6+8oveh5scaq9Bv2bJFV3mfOHGiBmNr166Vxx57TIMZdOFt3rxZ/v3vf8u3334r3bp1k7Nnz8o///yjj42JiZHhw4fLW2+9pdd0/vx5vQ/fYa6OAZAdBflfzAClZeUrgDYwACIiugjBz0+jHPPcd3wt4uNf6sMXLlxoDm4M48eP161///5SqVIlmTdvntx777163+zZs2Xw4MFSpUoVvX3bbbfle+yMGTOkZs2asnfvXmnTpk25X87UqVOld+/e8vLLL+vtZs2a6bnffvttDYCio6P1GpF5wjVFRERIx44dzQFQdna23HrrrbofkA2qCNgFZkfBAZcyQEb3F/7I8PPJexsycjgUnojI1dxwww2arbHcHnnkEb3P29tbsy+zZs0yZ3sWLFigmSHDoUOHNMvSqFEjCQoK0i4qQGBiDfv27dMV3y1dd911+rw5OTnSt29fDW7w/AjScK3IUEH79u01eELQg0zW559/LufOnZOKgBkgO6pyMQOEWp+zqZn6u7+Pl3h7emh3GAuhiYgsuqGQiXHUc5cBsidNmjQp9n4EOz179pT4+HhZunSp1tJYjhK75ZZbNABBcBEWFia5ubma+cnMzPuesLUqVarI1q1bZcWKFbJkyRLtgkN32aZNm7QuCdeMbjPc9+GHH8qLL74oGzZskIYNG4orYwbIjny9Pc1dXHHJ6eYACPuBcwEREV2E9Di6oRyxWan+x4C6mvDwcPnhhx80u4JMio9P3h/EZ86ckQMHDshLL72kmZaWLVtaPcOCc65ZsybfvjVr1mhXmJeXlzlTheJm1Prs3LlT5x76+++/9T4M60fGaNKkSbJt2zbx9fXVLj1XxwyQnQUF+EhaZo7EJ+cV9/lf7P4CZoCIiFxPRkaGxMbG5tuHgAIjvQwYDfbJJ5/IwYMHZfny5eb9GC2GYuTPPvtM6tSpo91eL7zwwhVdx+nTpwtNlohzPv3003L11VfLlClTtAh63bp1Mm3aNB35ZdQwHTlyRHr06KHXs2jRIs1CNW/eXDM9y5Ytk379+kmtWrX0Np4HQZWrYwbIQUPhjQxQgI+X+HnnReAMgIiIXA+GsSPQsNwwrL1gNxgKj+vWrZuvHgejvObMmaMjtdDt9Z///EeLk68EiqtRvGy5oVutU6dO8uOPP+rz4DnQxTV58mQtgAZ0c82dO1duvPFGDWwQqH3//fc6bB41SatWrZJBgwZpxgiZKgyhHzhwoLg6ZoAcNBIs/ryRAfKS7Ivz/3BFeCIi1zJz5kzdLgeBRXFDx9H1hODIkuWxKIq+3LBz1O+UBCPNCo42MyBYK+7xuG4EeBURM0AOzgBpDZBX3tvADBAREZF9MANkZ0EXh8KjDgj8LhZAAwMgIiIi+2AA5KAMkAEZIANHgREREdkHAyAH1QAZUASde7FvlzVARERE9sEAyAHD4C3lFUHnBT7sAiMid1YR1pci1/mcsAja4V1gnhYTIXIpDCJyP8akgMbyC0QlMT4nxufmSjED5KAV4S0zQMbCqMwAEZE7wmzEmIsGS0VAYGCgzj7saJgMEMtRpKenW2VVdipfWyPzg+AHnxN8XoxZrK8UAyA78/HylEA/b0nNyC6UAWIARETuKjQ0VH8aQZAzwBduWlqart3lDAFZRWYqQ1sj+DE+L+XBAMhBq8JfCoCwFlhe1xcDICJyV/jSwwzKWG4hKytLnAGuA7MgY4mI8na3kHXaGveVN/NjYADkoJFgMXJpIkRjLiCOAiMid4cvN2t9wZUXriM7O1v8/f0ZAFXAtmanpoNHgnE1eCIiIvtzaACEdNctt9wiYWFhmv6cP39+vvuxUBv2W24DBgy47HmnT5+ua6cgkuzSpYts3LhRnHUkGOYB4lIYREREbhQAXbhwQdq3b68BS3EQ8MTExJg3rFBbkh9++EHGjh0rEyZMkK1bt+r5+/fv71SFdZaTIaII2ugCYwaIiIjIPhxaAzRw4EDdSuLn51emau+pU6fK6NGj5f7779fbn3zyifz+++8yY8YMeeGFF8TZMkB+3qgByuvvZgaIiIjIPpy+BmjFihU6KqB58+by6KOPypkzZ4o9FnMIbNmyRfr06WPeh/kEcHvdunXibAuiYki8l6cHJ0IkIiKyM6ceBYbur1tvvVUaNmwohw8flvHjx2vGCMFMUaMEEhISJCcnR2rXrp1vP27v37+/2OfJyMjQzZCcnGwelmeN4ZjGOYyfVXw9JdeUK4G+3rrPw5SjtzOyLh1D1mtvsh22tX2xve2L7e16bV2Wxzt1AHT33Xebf2/btq20a9dOGjdurFmh3r17W+15Xn/9dZk0aVKh/UuWLNEZSa1l6dKl+hPLmDQweUjVLJMsWhQt6TmY/CsvC7Tw90Xiyfm2rNreZHtsa/tie9sX29t12rosy6k4dQBUUKNGjaRGjRoSGRlZZACE+5AZiouLy7cft0uqIxo3bpwWTltmgMLDw6Vfv34SFBRU7utGRIo3tW/fvub5DW6yuB+1P4uTtuvvffq116HxZN32JttgW9sX29u+2N6u19ZGD06FC4BOnDihNUCYLbQovr6+0rlzZ1m2bJkMHTrUvL4Ibj/xxBMlFlpjKwhvgjU/9MWdz9vbJF6enpoZyvXw4v9oVmLt94+Kx7a2L7a3fbG9Xaety/JYhxZBp6SkyPbt23WDqKgo/T06Olrve/bZZ2X9+vVy9OhRDWKGDBkiTZo00WHtBmSCpk2bZr6NTM7nn38uX3/9tezbt08LpzHc3hgV5owwvxELoYmIiOzHoRmgzZs3yw033GC+bXRDjRo1Sj7++GPZuXOnBjKJiYk6WSK6pKZMmZIvW4PiaBQ/G+666y45ffq0vPLKKxIbGysdOnSQxYsXFyqMdjaYDDEjK5dD4YmIiCp6ANSrVy9dAbY4f/7552XPgexQQejuKqnLyxlxRXgiIiL7cfp5gNyFOQDigqhEREQ2xwDISRizQaMbjIiIiGyLAZCTYAaIiIjIfhgAOQmuCE9ERGQ/DICcBIugiYiI7IcBkJPw4zxAREREdsMAyElUq+SrP1dHJjALREREZGMMgJxEv9ahEhTgIzGJ6fLbjlOOvhwiIqIKjQGQk6js5y3/ujZCf/9jd6wcTbjg6EsiIiKqsBgAOZHOEVXl6obVdHbsBduZBSIiIrIVBkBOpmuj6vozKS3L0ZdCRERUYTEActLh8FmcEJGIiMhmGAA5GR8vD/2ZncsAiIiIyFYYADkZH/OM0CZHXwoREVGFxQDISQMgdoERERHZDgMgJ8MAiIiIyPYYADlpDRACIAyHJyIiIutjAOSkGSDEPjm5DICIiIhsgQGQkwZAkJXDAIiIiMgWGAA5aRcYZHEoPBERkU0wAHIyHh4e4m3UAXFVeCIiIptgAOTUI8HYBUZERGQLDICckC+HwhMREdkUAyAnZHSBZTIAIiIisgkGQE7cBZbNLjAiIiKbYADk1OuBMQNERERkCwyAnJCv98UaIA6DJyIisgkGQM68HAYzQERERDbBAMgJeXterAHiUhhEREQ2wQDIibvAWANERERkGwyAnHxFeCIiIrI+BkBOiDNBExER2RYDICfkzZmgiYiIbIoBkBPyM+YBYgBERERkEwyAnHgpDM4ETUREZBsMgJy6BogZICIiIltgAOSEGAARERHZFgMgJ+TrzdXgiYiIbIkBkDNngLJZA0RERGQLDICceikMZoCIiIgqXAC0atUqueWWWyQsLEw8PDxk/vz55vuysrLk+eefl7Zt20qlSpX0mJEjR8qpU6dKPOfEiRP1XJZbixYtxCW7wLgUBhERUcULgC5cuCDt27eX6dOnF7ovNTVVtm7dKi+//LL+nDt3rhw4cEAGDx582fO2bt1aYmJizNvq1avFlXAmaCIiItvyFgcaOHCgbkUJDg6WpUuX5ts3bdo0ueaaayQ6Olrq169f7Hm9vb0lNDRUXBVHgREREdmWS9UAJSUlaZdWSEhIiccdOnRIu8waNWokI0aM0IDJFRdDZQ0QERFRBcwAlUV6errWBA0fPlyCgoKKPa5Lly4yc+ZMad68uXZ/TZo0Sa6//nrZvXu3VKlSpcjHZGRk6GZITk421yFhKy/jHKU+lylXck25kp6ZbZXndzdlbm+6Ymxr+2J72xfb2/XauiyP9zCZTE5RaILMzrx582To0KFFvqDbbrtNTpw4IStWrCgxACooMTFRIiIiZOrUqfLggw8WWziNQKmg2bNnS2BgoNjbuQyRn6I8xc9TZFQzZoGIiIhKA/XD99xzj/YYXS5WcPoMEIKfO++8U44dOyZ///13mYIfQHdZs2bNJDIysthjxo0bJ2PHjs2XAQoPD5d+/fqV+fmKew2oZ+rbt6/4+Phc9viElAxZOX+P+Hp5yqBBHcr9/O6mrO1NV45tbV9sb/tie7teWxs9OKXh7QrBD2p6li9fLtWrVy/zOVJSUuTw4cNy7733FnuMn5+fbgXhTbDmh7605wvwM4mnh6dgEBgKupEdo7Kz9vtHxWNb2xfb277Y3q7T1mV5rEOLoBGcbN++XTeIiorS31G0jODn9ttvl82bN8usWbMkJydHYmNjdcvMzDSfo3fv3jo6zPDMM8/IypUr5ejRo7J27VoZNmyYeHl5ae2Qq/D1zntb0DmZk+sUPZREREQVikMzQAhubrjhBvNtoxtq1KhRWpfz66+/6u0OHfJ3AyEb1KtXL/0d2Z2EhATzfagTQrBz5swZqVmzpnTv3l3Wr1+vv7vaTNDGXEDeXg69HCIiogrHoQEQgpiSarBLU5+NTI+lOXPmiKszhsFDVm6uBAgjICIiIredB8hdoObH+2IQlMXlMIiIiKyOAZCT4nIYREREtsMAyElhCDxwOQwiIiLrYwDkpIwusEwGQERERFbHAMhJcUFUIiIi22EA5OwBUDZrgIiIiKyNAZCTT4aIYfBERERkXQyAnHwuIA6DJyIisj4GQE6Kw+CJiIhshwGQk2IRNBERke0wAHL2LjAGQERERFbHAMhJsQuMiIjIdhgAOSl2gREREdkOAyAnXwqDM0ETERFZHwMgJ18KI5tdYERERFbHAMhJsQuMiIjIdhgAOSkGQERERLbDAMhJ+XpzNXgiIiJbYQDkpLgYKhERke0wAHLyACibi6ESERFZHQMgJ58JOpOLoRIREVkdAyAnxZmgiYiIbIcBkJPiKDAiIiLbYQDkpBgAERER2Q4DICfFpTCIiIhshwGQk+JSGERERLbDAMhJsQuMiIjIdhgAOXkXGAIgk4lZICIiImtiAOSkfC4uhYHYJyeXARAREZE1MQByUt6el94aFkITERFZFwMgJ54J2iMvCcT1wIiIiKyMAZCT8vDwEF/vvLcnIzvH0ZdDRERUoTAAcoFC6AyuB0ZERGRVDICcmJEBYg0QERGRdTEAcoUAiBkgIiIiq2IA5MT8vL30JwMgIiIi62IA5MTYBUZERGQbDIBcYUFUZoCIiIisigGQE/Pz4TB4IiIiW2AA5MSYASIiIqqAAdCqVavklltukbCwMJ34b/78+fnuxyKgr7zyitSpU0cCAgKkT58+cujQocued/r06dKgQQPx9/eXLl26yMaNG8UV+ZknQmQAREREVGECoAsXLkj79u01YCnKW2+9Jf/73//kk08+kQ0bNkilSpWkf//+kp6eXuw5f/jhBxk7dqxMmDBBtm7dqufHY+Lj48VVR4ExACIiIqpAAdDAgQPl1VdflWHDhhW6D9mf999/X1566SUZMmSItGvXTr755hs5depUoUyRpalTp8ro0aPl/vvvl1atWmnwFBgYKDNmzBBXw3mAiIiI3KwGKCoqSmJjY7XbyxAcHKxdWuvWrSvyMZmZmbJly5Z8j/H09NTbxT3GmTEAIiIisg1vcVIIfqB27dr59uO2cV9BCQkJkpOTU+Rj9u/fX+xzZWRk6GZITk7Wn1lZWbqVl3GOsp7LU3Il15QrqRnWuQ53caXtTWXHtrYvtrd9sb1dr63L8ninDYDs6fXXX5dJkyYV2r9kyRLtPrOWpUuXlun4/YkeEh/vIXsuxMmilH1Wuw53Udb2pivHtrYvtrd9sb1dp61TU1NdPwAKDQ3Vn3FxcToKzIDbHTp0KPIxNWrUEC8vLz3GEm4b5yvKuHHjtHDaMgMUHh4u/fr1k6CgoHK/FkSkeFP79u0rPj4+pX5cjaNnZf/qo9KgdhUZ1Ldpua/DXVxpe1PZsa3ti+1tX2xv12trowfHpQOghg0batCybNkyc8CDF4bRYI8++miRj/H19ZXOnTvrY4YOHar7cnNz9fYTTzxR7HP5+fnpVhDeBGt+6Mt6vkA/X/H08JRsU95jqWys/f5R8djW9sX2ti+2t/2Ut63L8tgrCoA2bdqkgQUKki0hOEEG5qqrrirVeVJSUiQyMjJf4fP27dulWrVqUr9+fXnqqad0lFjTpk01IHr55Zd1ziAjuIHevXvrKDIjwEEmZ9SoUXoN11xzjY4kw3B7jApzNSyCJiIiso0rCoAef/xxee655woFQCdPnpQ333xTA6HS2Lx5s9xwww3m20Y3FAKYmTNn6nMgeHn44YclMTFRunfvLosXL9YJDg2HDx/W4mfDXXfdJadPn9YJFFEsjewRHlOwMNqVJkLkYqhEREROEADt3btXOnXqVGh/x44d9b7S6tWrl873UxzMDj158mTdinP06NFC+5ANKqnLy1X4enEiRCIiIqeZBwj1MgULjSEmJka8vZ22rMiFF0NlAEREROTwAAijozByKikpybwPXVTjx4/XCm6yDi6GSkREZBtXlK555513pEePHhIREaHdXoDiZdTZfPvtt9a+RrdlFEHn5pokOydXvC8GREREROSAAKhu3bqyc+dOmTVrluzYsUNXascoq+HDh3OooA0CIKMQmgEQERGRdVxxwQ5WZsfoLLIdb08PLQRHoTi6wQJ9HX1FREREbhYA/frrr7p6OzI8+L0kgwcPtsa1uT0EPxgKn56VwzogIiIiRwRAmHwQ8+rUqlUr30SERX1pY0FSsl43GAIgjgQjIiJyQACEmZ+L+p3sMxkiAyAiIiLr8bySBcuw/MShQ4eseBlUHC6HQURE5AQBEGqAMAKM7DwXEJfDICIisporGlf9r3/9S7788kvrXQVdfjboLNZVEREROXQYfHZ2tsyYMUP++usv6dy5sw6JtzR16lRrXZ/bM9YDYwaIiIjIwQHQ7t27zYuhHjx40IqXQwWxBoiIiMhJAqDly5db/0qoxACIo8CIiIgcXAP0wAMPyPnz5wvtv3Dhgt5H1sMMEBERkZMEQF9//bWkpaUV2o9933zzjTWuiy7y44rwREREju0CS05O1nWpsCED5O/vb74Psz8vWrRIZ4omG3SBsQiaiIjIMQFQSEiILnWBrVmzZoXux/5JkyZZ7+rIPBM0M0BEREQOCoBQ/Izsz4033ii//PKLVKtWzXyfr6+vRERESFhYmBUvj1gDRERE5OAAqGfPnvozKipK6tevrxkfsi0GQERERE5SBI1Mz+rVq3VG6G7dusnJkyd1/7fffqv7yXr8vPMmQszI5kzQREREDg2A0P3Vv39/CQgIkK1bt0pGRobuT0pKktdee81qF0fMABERETlNAPTqq6/KJ598Ip9//rkujmq47rrrNCAi6+FiqERERE4SAB04cEB69OhRaH9wcLAkJiZa47qo4DD4LAZAREREDg2AQkNDJTIystB+1P80atTIGtdFBYfBMwNERETk2ABo9OjRMmbMGNmwYYOOBDt16pTMmjVLnnnmGXn00Uetd3XEeYCIiIicZTHUF154QXJzc6V3796Smpqq3WF+fn4aAD355JPWv0o3ZrkYKuZg4tQDREREDgqA8CX84osvyrPPPqtdYSkpKdKqVSupXLmyFS6JigqAEPxk55rEx4sBEBERkV0DoNKu9D5jxowrvR4qZhSY0Q3mY3GbiIiI7BAAzZw5UydB7Nixo2YkyPa8vTzF09NDcnNNGgBV8nP0FREREblZAIQC5++//16Xwrj//vt1JmjL9cDIdt1g6Zk5WgdERERE5Vem/pTp06dLTEyMPPfcc/Lbb79JeHi43HnnnfLnn38yI2RDHAlGRERkXWUuKMFor+HDh8vSpUtl79690rp1a3nsscekQYMGWgxNtpwLiOuBERERWUO5Kmo9PT11RBiyPzn8crZ5ITS7wIiIiBwUAGHhU9QB9e3bV5o1aya7du2SadOmSXR0NIfB2wgXRCUiInJgETS6uubMmaO1PxgSj0CoRo0aVr4kKmkyRCIiIrJzAIQV4OvXr6/rfa1cuVK3osydO9cKl0YGXy8v/ZnF9cCIiIjsHwCNHDmSSzE4ALvAiIiIHDwRItkfAyAiIiLr4roKLsD34vpfmewCIyIico8ACPMLodut4Pb4448Xm6UqeKy/v7+4MmaAiIiInGA1eHvatGlTvjmGdu/erUPw77jjjmIfExQUJAcOHDDfdvW6JWMBVGaAiIiI3CQAqlmzZr7bb7zxhjRu3Fh69uxZ7GMQ8ISGhkpFwQwQERGRm3WBWcrMzJTvvvtO5yAqKauDJTmwaj3mKxoyZIjs2bNHKsJM0AyAiIiI3CQDZGn+/PmSmJgo9913X7HHNG/eXGbMmCHt2rWTpKQkeeedd6Rbt24aBNWrV6/Y2a2xGZKTk/VnVlaWbuVlnONKz+XlYZJcU66kZ2Zb5XoquvK2N5Ue29q+2N72xfZ2vbYuy+M9TC60jHv//v3F19dXV6IvS2O0bNlSF3CdMmVKkcdMnDhRJk2aVGj/7NmzJTAwUBztUJKHLI/xkHqBIoPqMwtERERUlNTUVLnnnns0AYJ64AoRAB07dkxnoMYs0+jWKgsUTHt7e+vSHaXNAKH7LCEh4bINWNogbOnSpVq87ePjU+bHbzl2Tj79J0qa1qosz/ZrVu7rqejK295Uemxr+2J72xfb2/XaGt/fWKKrNAGQy3SBffXVV1KrVi256aabyvQ4jCDDgq2DBg0q9hg/Pz/dCsKbYM0P/ZWeL9DfVzw9PCU714P/E5aBtd8/Kh7b2r7Y3vbF9nadti7LY12iCDo3N1cDoFGjRmkmp+DyHOPGjTPfnjx5sixZskSOHDkiW7dulX/961+aPXrooYfE9YfBX5oOgIiIiK6cS2SA/vrrL4mOjtbRXwVhv6fnpTju3LlzMnr0aImNjZWqVatK586dZe3atdKqVStxVRwGT0RE5IYBUL9+/aS4UqUVK1bku/3ee+/pVpFwGDwREZF1uUQXmLvzu5gByspxiXp1IiIip8cAyIVqgDKYASIiIrIKBkAuVAOEbsBsrgdGRERUbgyAXCgAAi6ISkREVH4MgFyAt6eHGEufZWWzDoiIiKi8GAC5ACz8amSBMjgXEBERUbkxAHKxQmiOBCMiIio/BkAugnMBERERWQ8DIBfhw9mgiYiIrIYBkItgBoiIiMh6GAC52GzQXBCViIio/BgAudyCqCyCJiIiKi8GQK7WBcaJEImIiMqNAZCLFUFnsQaIiIio3BgAuQhmgIiIiKyHAZCL4DB4IiIi62EA5CL8OAyeiIjIahgAuYhLa4ExACIiIiovBkAuFgCxCJqIiKj8GAC52GKoLIImIiIqPwZALoIZICIiIuthAOQiOAyeiIjIehgAuQhfbw/9yVFgRERE5ccAyEX4ennpzwwGQEREROXGAMjVaoDYBUZERFRuDIBcbjV4BkBERETlxQDIRfh4XawBYgaIiIio3BgAuQh2gREREVkPAyBXGwafnSsmk8nRl0NEROTSGAC5WAYIsU92LgMgIiKi8mAA5GIZIGAhNBERUfkwAHIR3l6e4uGRVwjNOiAiIqLyYQDkQvw4FJ6IiMgqGAC54FB4zgZNRERUPgyAXHEyRHaBERERlQsDIBfC2aCJiIisgwGQCy6IyiJoIiKi8mEA5EJ8vC8uh8EMEBERUbkwAHIhfsZs0MwAERERlQsDIBfCGiAiIiI3CIAmTpyok/9Zbi1atCjxMT/99JMe4+/vL23btpVFixZJReFjsR4YERERVdAACFq3bi0xMTHmbfXq1cUeu3btWhk+fLg8+OCDsm3bNhk6dKhuu3fvloqAw+CJiIjcJADy9vaW0NBQ81ajRo1ij/3ggw9kwIAB8uyzz0rLli1lypQp0qlTJ5k2bZpUpACIo8CIiIgqeAB06NAhCQsLk0aNGsmIESMkOjq62GPXrVsnffr0ybevf//+ur8iLYjKLjAiIqLy8RYn1qVLF5k5c6Y0b95cu78mTZok119/vXZpValSpdDxsbGxUrt27Xz7cBv7S5KRkaGbITk5WX9mZWXpVl7GOcp7Lk8xSa4pV9IyrHNdFZW12psuj21tX2xv+2J7u15bl+XxTh0ADRw40Px7u3btNCCKiIiQH3/8Uet8rOX111/X4KqgJUuWSGBgoNWeZ+nSpeV6/O4zHhJ/2kN2ZMRJ1TMVo67Jlsrb3lR6bGv7YnvbF9vbddo6NTW1YgRABYWEhEizZs0kMjKyyPtRIxQXF5dvH25jf0nGjRsnY8eOzZcBCg8Pl379+klQUFC5rxsRKd7Uvn37io+PzxWfJ/jwGUndHSvt64XIoM51y31dFZW12psuj21tX2xv+2J7u15bGz04FS4ASklJkcOHD8u9995b5P1du3aVZcuWyVNPPWXehwbF/pL4+fnpVhDeBGt+6Mt7vl4tQnWj0rH2+0fFY1vbF9vbvtjertPWZXmsUxdBP/PMM7Jy5Uo5evSoDnEfNmyYeHl56VB3GDlypGZvDGPGjJHFixfLu+++K/v379d5hDZv3ixPPPGEA18FERERORunzgCdOHFCg50zZ85IzZo1pXv37rJ+/Xr9HTAizNPzUgzXrVs3mT17trz00ksyfvx4adq0qcyfP1/atGnjwFdBREREzsapA6A5c+aUeP+KFSsK7bvjjjt0IyIiInLJLjAiIiIiW2AARERERG6HARARERG5HQZARERE5HYYABEREZHbYQBEREREbocBEBEREbkdBkBERETkdhgAERERkdthAERERERuhwEQERERuR0GQEREROR2GAARERGR22EARERERG6HARARERG5HW9HXwCV3ZmUDFl+4LTk5ObKXVfXd/TlEBERuRxmgFzQ+fRs+WNXjCzff1rSs3IcfTlEREQuhwGQC4qoHii1gvwkKydXdhxPdPTlEBERuRwGQC7Iw8NDrm5QTX/fGHXW0ZdDRETkchgAuSgjANp1MklSM7MdfTlEREQuhQGQi6pXNUDqhPhLTq5JtkezG4yIiKgsGABVgG6wDewGIyIiKhMGQC7smoZ5AdDemGRJyWA3GBERUWkxAHJhdYIDtCssN9ckO0+wG4yIiKi0GAC5uHb1QvTnrhNJjr4UIiIil8EAyMW1Dw/Wn7tPJWtBNBEREV0eAyAX17BGZQn085bUjGyJSkiRhJQMeXXhXlm8O9bRl0ZEROS0GAC5OC9PD2kTFqS/7zyRJHM2RktUwgX5fVeM1gYRERFRYQyAKoC29fK6wbBA6raLcwJpRujMBQdfGRERkXNiAFQBtK0bLB4eeUGPkRWCPaeSHXxlREREzokBUAVQxd9HGtaodPF3b7m1Uz39ffdJjgwjIiIqCgOgCuKGFrXE19tTRlwbYZ4g8cjpFLnggAkSI+PPy7kLmXZ/XiIiotLyLvWR5NS6Na4hXRtV1yUyAOuExSSmy76YZLnq4pIZ9rD3VLK8u+SA1K0aIJMGtzZfDxERkTNhBqgCsQw22oQFm0eGHYw7L2siEyQzO7fExyenZ+W7jdFk+2PLVkf055684fcnz6XJofiUMj2WiIjIXpgBqqDa1A2WpXvjNPDBBn/ti5NHezUWP28v2XLsrFzIyJFb2odJdk6uzFgTJRuOnJWb29eRYR3ryfGzqfL6on06ueLYfs2k9cWAqiQxSWn56o5WHTwtzWpXKfJYnBe12iVliHANWOOsZZ28Yf5ERETWwgCogkLgUcnPW2uAAny9NNCIPpMqExbskaycXDGZRPx8PKVX85ryxT9R5sBl4Y4YqR3kL7/vjDHPLP3l6iiZPKSN3l53+Iy0qxcsYSEB5udKTM2UkEBf+WtfvN6uHewvcUnpsvnoObmnS7YE+ub/mJ1JyZC3/zwg/j5e8mz/5nqdBZ29kCmv/7FPMrJy5Zn+zRkEERGRVTEAqqBQED1+UEs5l5opTWtVluT0bPl05WGJvNgthVFjmD/o3SUHNdPi4+UprcOCZPvxRPnynyg9JjjQR4MUBDPvLT0oscnpkp6ZI7/uOCkPXd9I6oUEyC9bT+pCrC/e1FLWXsw03XtthMzacExrkJBVCgrwlkNxKVqoXTXQVz5acVhOn88wB1dP3thEAzSTyaRBlreXp3y/MVqDH8C5Jt7SWvcTERFZAwOgCiw02F83qFbJV7Mtu04maYFyrSr+2vV15PQFSfD1kqf6NNVlNd5ZckAOxp7XeYUe6t5IAn295L+L9snRhLxJFY1lN6b/HanzDSFgwbGfrDysNUbh1QKlRWgV6dG0pvyw6bgGL8g2wfID8VK/WqCeC+fJzM6RHccTZdaGaMEh+P3mdnUkOMBHth47J56eHhLg46WBFLJLA9qElup1xySlS3ya7dqViIhcHwMgN4IMSsf6VfPdRk0QupuMLq1HejbW5TRahQXpBiO7RsiyffHSu2UtHWn24+YTsmxfnAY/qDW6vXM92Rp9Ttcf69uqtmZzujauLj9vOaHH+Pt6SViwvwZb2BAwPdKzkT7vzDVHZfn+vK4z2HT0rMQl52WHBrQOlVpBfnoMsk7ouqtR2U8iqgdKnWB/OZ+RLWsjz8jplAydDLJB9UBdAuTvfXGSmegpI8u5FMj59Cw5GJcinSMutRkREVUMDIDcHLq4LOt5kH35v56N8x1zfdOauhnu6VJf64DQbdY8NK/IGZmf3i1ri5+3p3lyxn/3biqxSenSrUl1zeRsjDorS/bGSY9mNc1F1cjubDt+Tmt82tcL0aDou/XHpGYVPy3I9vXy1GJqBE7zt500XwPqhtKycszrna2wCKJyTSYJ8jHJ7E3HJaSSnzSqUVkzTwi8cHhIgI9mlwypmdmy6mCCbIs+Jx3CQ/R1HD6dIp//c0TOp2fLuIEtpFHNysW2IbruONyfiMi1OHUA9Prrr8vcuXNl//79EhAQIN26dZM333xTmjdvXuxjZs6cKffff3++fX5+fpKenm6HK3YfyPwUVLlAMTOOsTyuS6Pqulm68+pw3SxVreSrgRRGq8FTfZpp91l8cobEn8/QLjRjgkfUMiEjtO14oiSlZmkgdlvHOnJ4S6wsOXpOMrILZ4EwW3b78BAJCfSRE2fTZF9ssrneCDVSS/fFSXJalnbdoQvx2JlU3Z+Qkqk1VSgS7xAeLImpWbJsf7ycOJcmN7WtI31a1mKdEhGRi3DqAGjlypXy+OOPy9VXXy3Z2dkyfvx46devn+zdu1cqVcpb+qEoQUFBcuDAAfNt/nXuWpCFsYRsz83twsy3Ubt0/Fya+Pt4Sp3gvOzViC4mSUzL0uxOTk62HBKRWzuGybFzGTojNjJReR8DD83qrD6UV7BtQF0UurpWHjytgRRc37SG3H1Nfc1IYfSbpT92xeS7/dPm47L2cIKM6tZAGltki1AXtfnYWe3mu+vqcGlSq+hpAYiIyL6cOgBavHhxoexOrVq1ZMuWLdKjR49iH4eAJzS0dAWz5HqQZTHWPjOgSwuF3pCTI+LlIdKzWU3x8fHJ102F4Al1PRjtlpGdI+FVA6VBjUANWnB//9ahGhyhCw5ZImNKgawck1Sv7KsBFiaIRDG5t6eHduchYzVv60md/BFzJw1qW0e7BnccT5KNUWc04IL5207pzxZ1qmiheIPqlZgxIiJyEKcOgApKSsqbq6ZatZKXdkhJSZGIiAjJzc2VTp06yWuvvSatW7cu9viMjAzdDMnJebMfZ2Vl6VZexjmscS4qf3s3rRmgmyVkGAGdbj2bVsv3+K4NQ3Qz3NCseqG6nw51q8gPm0/Ihqiz8tuOk/LbjkvnxtD/nk1r6NQEP245IXtOJer+Kn4+0r91Lb3Pzyevu8/V8LNtX2xv+2J7u15bl+XxHib8S+4CEMwMHjxYEhMTZfXq1cUet27dOjl06JC0a9dOA6Z33nlHVq1aJXv27JF69fJWSS9o4sSJMmnSpEL7Z8+eLYGBgVZ9HVSxRZ0XWRfnKagoCq9kkgaVTRJeWXTW65QskWMpHnIqVeTUBQ/JuLgyCbJV3jortkizYJNcVcOkxdqbEzwkOVOkZx2TBLjUnypERI6Rmpoq99xzj37/oxymQgRAjz76qPzxxx8a/BQXyBQXDbZs2VKGDx8uU6ZMKXUGKDw8XBISEi7bgKW9hqVLl0rfvn3NXTJkO67Q3uiK23D0nCzaFavD+C3VDQmQ1MwcLbgGdJc9dWOTfCPXnIUrtHVFwva2L7a367U1vr9r1KhRqgDIJf6ufOKJJ2ThwoWaySlL8ANoyI4dO0pkZGSxx2CUGLaiHmvND721z0eu2964rF4tQuX6ZrV1VmyTmORUYpp8u+6YxCTlBUS1gwIkKQ1zEV2QP/aelqEd64qzcua2rojY3vbF9nadti7LY506AEJy6sknn5R58+bJihUrpGHDhmU+R05OjuzatUsGDRpkk2skKg/Mpm3M1o0RbU1rV5G5W07oPEo3taujxdqfrzoiC3eekuxck9zYopa52JuIiK6cUwdAGAKPOpwFCxZIlSpVJDY2VvcHBwfrvEAwcuRIqVu3rs4ZBJMnT5Zrr71WmjRpovVCb7/9thw7dkweeughh74WotII8veR+667FOhf26i6zkGEYfQYer94d4zO5v1wj0Y6Ci0jO9c8oWVJ3W0YtYbJJ1GITURETh4Affzxx/qzV69e+fZ/9dVXct999+nv0dHR4ul56R/1c+fOyejRozVYqlq1qnTu3FnWrl0rrVq1svPVE1nHiC71pWWdKvL3/njZH3Neu8We/3mnpGRk61IjyCIN6VBXBrUNLXLOKyxJsnRvnGaUbu1Uti5kIqKKyqkDoNLUZ6NrzNJ7772nG1FFgaCmc0Q13U4mpunkim8t3q/BD+Dn3K0n5MS5VM3yxCan67poVzWopjNmrzp02rzO2rCOdTkxKBGRswdARCSFRojBSze30vXVAn29ZN2RMzJ7Q7SutYYNEOM8H+Cj3WfGMh9YSgTBkTF7NhGRO2MAROTCgRDc0LyWhAUHyPztJ7UuKD0rRxeP/eKfKC2cBj8fTw2EdhxPZABERIQVBBx9AURUflh64/kBLeTpfs3lP32b6UixhJQMSUzNlKAAH60Rgu3H82ZTJyJydwyAiCqYQF9veaD7pZFkGDqPhV4hMv68Fk8TEbk7BkBEFVDLOkG6+nyH8BDp3bKW1KjsJ/WqBgjGFew8kbcWGRGRO2MARFRB9WsdKk/2bqoZITBWt1954LScvZC3zAYRkbtiAETkJq5pWE3XE8PIsBd+2SmzNhyTpFSuck1E7okBEJGbqFc1UJ4f0FyahVbRuYP+3hcvJxJTdWJF1AVh9FhBLrJWMhFRmXEYPJEbaVKrijzXv7nsjz0vB2LPy6qDCbL5aN7cQYDJE0d1i5CsbJN8uSZKJ1d8pl9zCbMYdl8SBEy/74oRXy9P7YIjInJWDICI3AxmgkaRNLaPVkTmuw/BUPTZCzpnEDJDMGfTcRnbt1mxAU+KRS/a0TOpMm/rSf0dI8+qV/az5UshIrpiDICI3NhjvZpoEIOerqgzF+STFYd1xmioE+Kvv+85mSS7TiRJ23rBhR6/dF+8/HTEU8KPnJEezUNl/ZEz5vt2nkiSG1rUsuvrISIqLdYAEbk5ZIRQHN24ZmWZMLi1XNekhvRuWVteuqmVziEEP24+LsnpWea1yOD42VSZvz1GskxYcd6kdUUbLAKg7cc53J6InBczQERkVtkv/ySKt7QPk7WHz8ipxDT5z5ztui840Ef+dW2ELNh2UrJzcyWiskm6N6kue08ly/n0bPHx8pSsnFzZH5ushdX+Pl4OfEVEREVjBoiIilXJz1tu61zPfBvBDYbOT/87Uk6cS5Mqfj7SI9SkWaR1RxL0mB7NaurEi8gK7YtJLvK86FJbvj+eo8yIyGGYASKiEvVsVlMLmv28PSXXZNIi56V74/S+kV3ry8mdxzXTs/VYXpdX18bV9eeyfXG6+GrH+nnLcBiQTfrw70PaZYbV7Ls0yju+IByHLFKvZrW0iw7iz6frxI7IVBERlQf/FSGiy7IMOO6+pr50a1xD0rJypFF1fzm5M2+RVXR71Q72lwbVA+VCRrYGQCiETs3MFi9PD/Hz9pLcXJPMXHtUgx/4ecsJDZB8vfMno5EZmrY8UuKS0sVDPLSYGgHRpN/2SLVKfjJ5SGvNRpUGhvu/t/SgLg3iDkXZaGMEqt6lbB8id8X/Q4iozOpXD9QV6A1VA310lNh1jWtodxju8/Px1KH0T87eJo99t1XeXXJAZm2MlsPxKVoXFBLoq0tyGNkkS7tOJmnwA8sP5HWV/bUvTrvV4pPTdTmP4iBLlJZ5aVJHPA7B2dJ9l54HXXN/7Y0zd8Gdu5ApE3/dIwt3nhJX9+7SA/L8L7s0CCWi4jEDRETlhoCnTXg1821kZ9B19eeeWPM+FEljg9uvqif+3l7yxT9H5PddpyT6bKqcS82U8GqBcu+1ERqcGE6eS5NtxxNlbeSlEWYIVLo3rVGowBpdcR/8dUiQYPp37yZSNdBX640AAVVsUrpUq+Qr05dHapBUtZKPdI6oJn/vj9dRbafPZ8iA1qHFZk/iktO1KxDBW1kgACttxqo8EFDujzmvv++NSZarG1x6T4goPwZARGQTd14dLsM61dU5hhLTMrXo+Z9DCdKsdhXp1aymHoNusqiEC+bZqJGRQVfXnlPJ4uEh0iosWOch+vKfKA0iECBlZOdqFmjRrhjpVL+qXMjM1nN6e3rIN+uOapCDAAUF3Mgk4XGG7cfPaVBkZIhWHkyQjuFVdaSbEUAdiDsvrcMKz3mE55ywYI9U9veW129tW+qABoHV238ekF7Na8qwjnU1Q2YrWOfNMsvlDAFQdk6uLNwZI23qBulM5ETOggEQEdmMESTUquIvd11dXzcEOUYQ8H89G8vqyNNSyddbMzM4DkERdAgPkcHt62oAZKxT1rdVba0XwoSNv++M0U3PH+QvbesGy4YjZ/Xcj/RsJEH+PrL56Dnz0H2MXtsWnZiv3mjvqSRZefC0JKZmmvfhGARACFwQnF3fNK9b76998RpMobsM5zWKvQtCYIa11e7oXO/i4+K0Oyr+fIZNgx84FJ+X/YHiRuDZ2/ojZ+W3Hadk87Gz8urQtld8HnxuZqw5qp+FR3s2NhfGuxPL/3eo/FgDRER2ZfkPeM0qfjKsYz1dNwzrkFWv7GvOxvRpVVtrjRrXqqy3q/h764r2V0VU1QVdjX3I9CA7YwROt3WqK01rV5GM7BzZeSJvZNqILhH68/DpFHNgUK9qgGanvt8YbZ752pjAEV80szdGy9drj8rcrSe1kBuBmgF1SUXZcuys/LLlhPy5O1Y2Rp3VQAhBmb6elsUXYGNySRSEb4vOC9gA13A+3WKdEa1nipe4tOIXqT0UdykDhFm8E1LyZvW2FnQBoq3LYs+pvC7ImMR07aK7UrHJ6bI2MkG2Hjuns5a7o09WHtHuW6zRVx74XKZZ1Mm5KwZAROQ0UjNzpEVokHZ1Na+dF+Tc0i5MR5EN6VBXM0oIoJ7t11w+vKejvH93R3nztnbSv3WoHoMgakCbvEVYd59M0sACQVWn+iF6TsQN2FrUqSKDO4TpccaItPu6NdDzI8ODYORg7Pm8WqbmNWX1oQRdHw3zG+F5UMh9rMCXML5UvlufF0zB/O0nZcWBvKwRnhszbRfn1x2n5I9dMfL5P0f0PPD9xuPynx+261QCgGDmpy0nZcExTzmdkpmvyxA/EaQZX4y1gvysngVCMDb5t70y6be9Oit4aeC6UItkwHtypYxaLkAXqbvB+4sAGQEgPoPlgZq3MXO2yYLteev2WROC3DWRCS4xxxe7wIjIaSAjNKZPUw1cjEwRRpd9NvKqfMeh+wPzAUGAr5e53gh1QMbjjO4vBEXY17F+iHZrQfcmNaV9vRAJDvDRkWoIUFCf0josSDNAi3fnFW/f3L6O1gzhCwMGtg3VYfXI7vyxO1aDmv0xyVI7yF9iktIlOS1LM0kp6dmagZm/LW9UWe+WtfJlvtB1hzmO7rwqXAMy4/kQZKEAHPMu/b0fo9TyCr7bh4fIigOnxSQmqRuILsW8AOenzSe00BxBHxa3xfFoQ2TKFu6I0QDo+qZ59VYlQZCGoA9ZMcvj8T4YXYbIzBldkX/vi5ehHesWea5v1x+T6DMXZEyfZnImJUPbwrD7VJJOlHklUM9lmVUa3D5Mz48MHpZuwesvz9QBP289oZ8HBNPlgTZDEIsuXWtC1yyC9bCQAKkTHFCucyGozsk1lfkaEYThjxT8IVAcBPH44wGfqV7NnXvaCWaAiMjpFJwXqDSM7BDgr8/0LARRol1mgIJpCPTzlk4RITrSC11v0K9Vbf1pOWkjsij4MtxxIlFHh+FxqPtBMAObos7KnI3RGjAhCEF3G57v/usa6hIixnWgi65Lw0v1Qghw5m49oSPi/vv7Pu3SwHFG1gY1Q3M2RWswA0dOX9Cg659DeV1wrarmFXVvOnrWPMruzz1xWssETWpVNgcD+2LOa5CAGpxDcZfqgwpCgTquC11+CC5wPQhknvx+q6w7fEZvG+cHBIRGMGT5lz5qplbsj9drRpekkampevGLFq8ZwUZZaXF67KXrPxx/Qbtw5m8/pYEBrtvI5BWE61t7OEHXsSvO+qgz2m3546bj+bqXUMBtCW0Zk1T8efBc7/11UJ7/ZadEWtRjWcOlgD7/xKKlgddk1Lnh59GEvOxlu7ohpT4HXtu7Sw7K+Lm7zH9IFIQsJYIf2BCV1/XrzBgAEVGFg0AImaR37+ggDWtU0n3I8mDfM/2a6aSM0L91bZl6Vwfp1qSG3m4XHqxBDAy/pr4GVcjsIOOEGbHxOGR9mtTO685CjRKWCunVopYGHijyxv04Fl1vgIyHEdDhL28ENxAa7K9/JeNLA1ms5we00OwRvtgxlB3Pia46+HTVYc2kIBsVUVn0mr5aE6X3YVQavpzQNQKof8I14NqRkcKcQPO3nZQ3/tiv0w4g41Xwr3qM0gLEMp+vOiLfbYjWQAbzLs3acEw2Hzun0whgbie8LhR1o5sDo85e/X2fuWsLBeCW9UpGTRMCTLxGvLbS1u+g3uf1RfvkYNx5DX4Q4OC5ESji9SIANBbfRYC61aJ+yhICN4wi/OCvg+YgCd00qONCYIUgZ8HFTB0Y2bgjp1PkpV/3yqmL3/W49ikL9+pIwOICAGQGEQAgyJu3rfjuJQRSS/bEmoPIy8F7ZNRSWY7sQ/fnS/N35Wv3ghD4Yo6r1xbt09dqZNIa1KikgwNK60jCBQ2c0IaW01sUfP2Wz1tczRfapzzdodbCLjAiqrAK/gPfrl5IoUAJ3R4GjBzDyDR8MRnHDmpbR7o2qi5eXh7mx/ynTzPt5iiuKwDZpUd7NdEv6EFt87JM0WdSNZBBkIE5jFBz9PuuGFl18LTOUo2h+6h3+mzVET0etUfXN6mps19jBBv0bFpDso5Fy8crj2h3GQKkUV0byCsL9piH+zetlRf8NKtdWTMwCBbQtYUMCLI5xgg2ZLLqVQ2UP3bFakCDgAxdiFjjDcEP4JqQMTCu6ZoG1SSieiX5bv0x+WXrCb0GQC0JulOMgMcYdXfkdF73V5u6wXIoPkWDNHzxNboYlBY1ognXi/opdOEBMmSY5gAw0g8RanxyvPyw+bh+GeMceAyCAGT7LM+J9/HX7XnBzZmUTJ0GAdMeIBjC60RtF0YbIgj19/WS9MwczVz0aVlbPlpxWL/A96TlnQ8TaWKxX8Drf2Fgi3zPZRSyGxDEInjDtaP4HvNHob0RzLyz5IB2kR4/lyYPdm94MUt1RgPloqYuQJYRrxUBMrrAjHb6Zt0xLS5HsIVuU3TF4n1Gtyw+Y14eHprJw2cOrx8ByvbovJqy9uGlz/6AMTgBcJ7bO9crNB+WEZCiRgnXi+ktjCyrpd92ntL3BaM6MbO8ozAAIiKyUNQXkNGFY8AEjJdb5R6ZJyP7BJX8vKRmZT+p4u+jkz3iy/PmdmG6WT43usDOpWZpNxoCMox4Q1YBXyrXN60uq6LzuvXWRZ3TYA3HDGpXRxZsO6nZoDrBeaPZ7rgqXGoePK3HoksMGY3ZG6K1mwpBF7ab2tUxz8SNLzR8gSLLgS9zZLYQcExeuNfcbYVsVt2qARqgICOF7//rmtSQWzvV0y9/fNGi1qpDeFVzhgpfkrgmBEEIgJCRQeCRmJYl1zaqrgEiMmvI4iCoxDHrL36RBgX4aBbLyG7hHLgSBGgIVuDuq8P1uRFgIguES8XUCSOujZAle+PyZbxwG/NIIfgBZDSM7iCMHtxy7JwGLsiW6dIuQf7SOdikASIyNoDXjMzX6sgErZfCaENkh5BRQsCEz0qL0CoabCIwRDYOwVndkAB58aZW8vmqKA1+jCxXl4bV5OiZC7rGnn62+nhp3ZulLUb3V8Slz+a6I2fM3Zpaw7TlhNzaqa5mzXA9CFjwvmMiUcOi3TEaCEGHAn8MoO0RMCFTVxA+D0Zwg5GXCATRDYr3HfdhWRw8Hu2Kz+nN7cP084hAqWAAhEwWumSNDKojMQAiIrKD6pX95IWBLbWQubiZptHtNX5QS/0SN0b63NS2jn7RIfhA8IQv4MHt68hN7euagzDMXo1sR+OalcxZCWNWbUOjmpXlxZtaysG4FA2y0B2HWh180aP7DpkQPPalm1vJ2ZRMnbgQtwe2CdWibZwPAR32IWux6eg5ubFFLd2HLh0jaEHGrH61QP3yR0DQKizvPG3C8uqSjGyWEQBgMyBzVa2yr7bDyK4ROhJw0sK9GuygPfCFjnXOjKwPMk3IlMWdT9fC7I+WHzafC8cu3p2XRUKRPAKEyLgUzZhAz+Y1ZcfxJM1wIZPXo2lNnYcKARDaBN19j/VsJNvWHNXuPAQVyKQhe4bi8x82HZcle+K0JshywNNtnepJ09qVNZOEcxkzc6P90BWJWjFvL7RHsGZ2Pl11RFItli35am2UTB7SRvy9PXUEHbr/jG4rZHkAWSTUKwECUNQ4IUjE6ERjeLtlcIcMI4JW47UjKA2vFqBZGlwPJijFun34WKKgvHeLWhrMoKsVnwP8zJs53VfPhXm4UJSPwAfzPOG9RfE9IGhGF/Cv209qsI1rwGSlubn4w8FTu1jRXvg8Y01BR2IARERkJ0X9dV0Qvtwv9raZsx6oU6rs6y05OZe+KC0zUOg6wYiy0pwby5Zgw7D2cxey5OctxzXbZAROyFRgM2C0Fbq3MD2BcQy6By27ExNSMrUrsXaQnwZacE+X+tpNhi9TIwDElyfm88GXJBbYxUg6BGIIeGpU9tUMGTI36JIyZuN+4LqG8vGKw9plY7xmBHrIwmCUEYJJFKujJgldcugmwvlXX7yNWhfUIOGLGBkJZHMQOOFaEKwhiLmuSXU9D0YBNqpZSb+4UcyOzNW6bBSJ5xWAD+5QV9rXC5Y1kWe0/iYtMy+7gmbBteGar22UN+oQE2hizToUwSOYQ7Zj/NzdevzIrg00mHllwW5zRgbdQTsvroH3/tKDcjY1M1+wiAAKARhgdCGyMFh8GOdGQIXnQrYLdVIP92gkX64+qnM2IbjFuRNTs8y1O+211s1Dlu6J0WDOkJ0jWgyOzfDzFg8JudiVjK7gzvWr6nPguhF0AjJY6OKELo2q62cBASiK3pFRLAivBTV2jsYAiIjIyaGbC3KsOHcdsknYxvZrXuJxCAwuN5wZAdUbt7Y118gYI+osR9VBwe4QFGwjs4AAznJuG3Q9GRAovHNHOw0kDPd2jdDsDb7YARmc/w5tq9khBFqAgAj1MOjaw5c9jjWKdBHUodvNr7KXBmoGre/q20yS0/JqorKysuRoiodk5OZIRPXKOp8Ujnnyxiay8ehZDRQR8AX5exeqZ7r76voafLSuE2yuRcPjUBeErA082L2R1jjhNSIgw/QFry3arwGY0QXYrl6w1hEhADQyXwh4EDT+qwuCH0+dIwu1XXj9mH4gr7utpdbg4Nx4HCYWRR0TusswBQRc07C6BoDoikTNEAJPZMrQbYZic2SK0P2KubEA143nRffqrPXHpHlokAZ6yP6gVg1ZMwRXgAyPsfYf7kcNFAI0tPtjvRpf0UhPa2MARERE5YYv4oK1UtbKihUstkUxMTZLBZ8bX9aoqTK+aBGo9GtdW4O07hcDkKJgfiljjiloEWKSoVc3Fy/vS0EOll6xrN0qCp63YBcPghjL4mMEju/f1cG8rAeuERkd1CJ1aVRNr7/gmnO4BmT7UKhtzOODjMuUoW3MvwMybJaBK45FwTwyV8gqGvveuaO9+flRYI3sGeqkjGwbMnToPkNXJ4JCwHUVrJV7/66OcvxcqnmEJe6vOzRAa4YQaDsjBkBERFQhFcwyYJqCK4FuMR8f23yJF1zTDJmT0kwWWXASQ8vRjMVBludyz+/h4ZGve7Vg0FZSWxec7dwYseasHJ+DIiIiIrIzBkBERETkdhgAERERkdthAERERERuhwEQERERuR0GQEREROR2GAARERGR22EARERERG7HJQKg6dOnS4MGDcTf31+6dOkiGzduLPH4n376SVq0aKHHt23bVhYtWmS3ayUiIiLn5/QB0A8//CBjx46VCRMmyNatW6V9+/bSv39/iY/PW4StoLVr18rw4cPlwQcflG3btsnQoUN12707bxE6IiIiIqcPgKZOnSqjR4+W+++/X1q1aiWffPKJBAYGyowZM4o8/oMPPpABAwbIs88+Ky1btpQpU6ZIp06dZNq0aXa/diIiInJOTr0WWGZmpmzZskXGjRtn3ufp6Sl9+vSRdevWFfkY7EfGyBIyRvPnzy/2eTIyMnQzJCfnrWCLlYCxlZdxDmuciy6P7W0/bGv7YnvbF9vb9dq6LI936gAoISFBcnJypHbt2vn24/b+/fuLfExsbGyRx2N/cV5//XWZNGlSof1LlizRbJO1LF261Grnostje9sP29q+2N72xfZ2nbZOTU2tGAGQvSDDZJk1QgYoPDxc+vXrJ0FBQeU+PyJSvKl9+/a12YrCdAnb237Y1vbF9rYvtrfrtbXRg+PyAVCNGjXEy8tL4uLi8u3H7dDQ0CIfg/1lOR78/Px0M5hMJv2ZlpZmlQ893lhEpThfdnZ2uc9HJWN72w/b2r7Y3vbF9na9tsbjLb/HXTYA8vX1lc6dO8uyZct0JBfk5ubq7SeeeKLIx3Tt2lXvf+qpp8z7EFVif2mdP39efyILRERERK4F3+PBwcGuGwABuqZGjRolV111lVxzzTXy/vvvy4ULF3RUGIwcOVLq1q2rdTwwZswY6dmzp7z77rty0003yZw5c2Tz5s3y2Weflfo5w8LC5Pjx41KlShXx8PAo92swutRwTmt0qVHJ2N72w7a2L7a3fbG9Xa+tkflB8IPv8ctx+gDorrvuktOnT8srr7yihcwdOnSQxYsXmwudo6OjdWSYoVu3bjJ79mx56aWXZPz48dK0aVMdAdamTZtSPyfOV69ePau/Fryp/J/Iftje9sO2ti+2t32xvV2rrS+X+TF4mErTUUbljmzxhiQlJfF/Ijtge9sP29q+2N72xfau2G3t9BMhEhEREVkbAyA7wAgzLOVhOdKMbIftbT9sa/tie9sX27titzW7wIiIiMjtMANEREREbocBEBEREbkdBkBERETkdhgAERERkdthAGQH06dPlwYNGoi/v7906dJFNm7c6OhLcnqrVq2SW265RWfzxGzcmMzSEmr3MTlmnTp1JCAgQPr06SOHDh3Kd8zZs2dlxIgROqdESEiIPPjgg5KSkpLvmJ07d8r111+v7w1mIX3rrbfE3WAW9auvvlpnPq9Vq5YuO3PgwIF8x6Snp8vjjz8u1atXl8qVK8ttt91WaM09TEqK2dcDAwP1PM8++2yhNX1WrFghnTp10pEeTZo0kZkzZ4q7+fjjj6Vdu3bmCd+wTM8ff/xhvp9tbTtvvPGG/ntiuVQS29t6Jk6cqO1rubVo0cJ52xqjwMh25syZY/L19TXNmDHDtGfPHtPo0aNNISEhpri4OEdfmlNbtGiR6cUXXzTNnTsXoxRN8+bNy3f/G2+8YQoODjbNnz/ftGPHDtPgwYNNDRs2NKWlpZmPGTBggKl9+/am9evXm/755x9TkyZNTMOHDzffn5SUZKpdu7ZpxIgRpt27d5u+//57U0BAgOnTTz81uZP+/fubvvrqK22D7du3mwYNGmSqX7++KSUlxXzMI488YgoPDzctW7bMtHnzZtO1115r6tatm/n+7OxsU5s2bUx9+vQxbdu2Td+/GjVqmMaNG2c+5siRI6bAwEDT2LFjTXv37jV9+OGHJi8vL9PixYtN7uTXX381/f7776aDBw+aDhw4YBo/frzJx8dH2x/Y1raxceNGU4MGDUzt2rUzjRkzxryf7W09EyZMMLVu3doUExNj3k6fPu20bc0AyMauueYa0+OPP26+nZOTYwoLCzO9/vrrDr0uV1IwAMrNzTWFhoaa3n77bfO+xMREk5+fnwYxgP8x8LhNmzaZj/njjz9MHh4eppMnT+rtjz76yFS1alVTRkaG+Zjnn3/e1Lx5c5M7i4+P17ZbuXKluW3xBf3TTz+Zj9m3b58es27dOr2Nf6g8PT1NsbGx5mM+/vhjU1BQkLl9n3vuOf3H0dJdd92lAZi7w+fwiy++YFvbyPnz501NmzY1LV261NSzZ09zAMT2tn4AhD86i+KMbc0uMBvKzMyULVu2aPeM5TpjuL1u3TqHXpsri4qK0nXhLNsVU6ije9FoV/xEtxcW0TXgeLT/hg0bzMf06NFDfH19zcf0799fu3/OnTsn7gpT0UO1atX0Jz7DWVlZ+dobae369evna++2bdua1+gz2hLT2+/Zs8d8jOU5jGPc+f+FnJwcXbAZCzyjK4xtbRvodkG3SsE2YXtbH0oRULrQqFEjLUFAl5aztjUDIBtKSEjQf+As30zAbXyB05Ux2q6kdsVP9B9b8vb21i91y2OKOoflc7ib3NxcrY+47rrrzAsIoy0QJCKgLKm9L9eWxR2Df9zS0tLEnezatUtrIFDD8Mgjj8i8efOkVatWbGsbQIC5detWrXUriO1tXfgjFPU4WLActW74YxU1llid3Rnb2ulXgyci+/6lvHv3blm9erWjL6VCa968uWzfvl2zbT///LOMGjVKVq5c6ejLqnCOHz8uY8aMkaVLl+pAB7KtgQMHmn9HoT8CooiICPnxxx91sIqzYQbIhmrUqCFeXl6FqtxxOzQ01GHX5eqMtiupXfEzPj4+3/0YSYCRYZbHFHUOy+dwJ0888YQsXLhQli9fLvXq1TPvR1ugOzcxMbHE9r5cWxZ3DEZCOeM/jraEv4QxeqVz586amWjfvr188MEHbGsrQ7cL/h3AiCFkgLEh0Pzf//6nvyNzwPa2HWR7mjVrJpGRkU752WYAZON/5PAP3LJly/J1MeA2+vvpyjRs2FD/J7BsV6Q/UdtjtCt+4n80/ANo+Pvvv7X98VeJcQyG26Nf2oC/FPHXedWqVcVdoM4cwQ+6YdBGaF9L+Az7+Pjka2/USaFv37K90a1jGXSiLfGPErp2jGMsz2Ecw/8X8v5dyMjIYFtbWe/evbWtkG0zNtQFojbF+J3tbTuYduTw4cM6XYlTfrbLXDZNZR4Gj9FJM2fO1JFJDz/8sA6Dt6xyp6JHbWAYJDZ8TKdOnaq/Hzt2zDwMHu24YMEC086dO01Dhgwpchh8x44dTRs2bDCtXr1aR4FYDoPHqAQMg7/33nt1CDLeKwyvdLdh8I8++qhOKbBixYp8w1dTU1PzDV/F0Pi///5bh6927dpVt4LDV/v166dD6TEktWbNmkUOX3322Wd19Mf06dPdcqjwCy+8oCPsoqKi9LOL2xiduGTJEr2fbW1blqPAgO1tPU8//bT+O4LP9po1a3Q4O4axY2SpM7Y1AyA7wDwFeNMxHxCGxWNeGirZ8uXLNfApuI0aNco8FP7ll1/WAAYBZu/evXVOFUtnzpzRgKdy5co6jPL+++/XwMoS5hDq3r27nqNu3boaWLmbotoZG+YGMiCwfOyxx3S4Nv7xGTZsmAZJlo4ePWoaOHCgzqWEf/Twj2FWVlah97VDhw76/0KjRo3yPYe7eOCBB0wRERHaBvjHHZ9dI/gBtrV9AyC2t/VgOHqdOnW0DfDvKW5HRkY6bVt74D9XnuAiIiIicj2sASIiIiK3wwCIiIiI3A4DICIiInI7DICIiIjI7TAAIiIiIrfDAIiIiIjcDgMgIiIicjsMgIiIiMjtMAAiIpd1+vRpefTRR6V+/fri5+ena8T1799f1qxZo/d7eHjI/PnzHX2ZROSEvB19AUREV+q2227TFaa//vpradSoka4KjYUSz5w54+hLIyInx6UwiMglJSYmStWqVWXFihXSs2fPQvc3aNBAjh07Zr4dEREhR48e1d8XLFggkyZNkr1790pYWJiMGjVKXnzxRfH29jZnjj766CP59ddf9fxYzfqtt96S22+/3Y6vkIhsiV1gROSSKleurBu6uDIyMgrdv2nTJv351VdfSUxMjPn2P//8IyNHjpQxY8ZoAPTpp5/KzJkz5b///W++x7/88suaYdqxY4eMGDFC7r77btm3b5+dXh0R2RozQETksn755RcZPXq0pKWlSadOnTQThEClXbt25kzOvHnzZOjQoebH9OnTR3r37i3jxo0z7/vuu+/kueeek1OnTpkf98gjj8jHH39sPubaa6/V50BmiIhcHzNAROSykKFB0IKuqgEDBmh3FYIUZHSKg4zO5MmTzRkkbAiikCVKTU01H9e1a9d8j8NtZoCIKg4WQRORS/P395e+ffvqhm6rhx56SCZMmCD33XdfkcenpKRo/c+tt95a5LmIyD0wA0REFUqrVq3kwoUL+ruPj4/k5OTkux8ZogMHDkiTJk0KbZ6el/5JXL9+fb7H4XbLli3t9CqIyNaYASIil4Sh7nfccYc88MADWvNTpUoV2bx5s47WGjJkiHkkGIbFX3fddTpPEEaNvfLKK3LzzTfr3EEY1YWgB91iu3fvlldffdV8/p9++kmuuuoq6d69u8yaNUs2btwoX375pQNfMRFZE4ugicglYeTXxIkTZcmSJXL48GHJysqS8PBwDYrGjx8vAQEB8ttvv8nYsWN1+HvdunXNw+D//PNPrQPatm2bZolatGihXWeoBTKKoKdPn64jzFatWqXD4N9880258847HfyqichaGAARERVQ1OgxIqpYWANEREREbocBEBEREbkdFkETERXAygCiio8ZICIiInI7DICIiIjI7TAAIiIiIrfDAIiIiIjcDgMgIiIicjsMgIiIiMjtMAAiIiIit8MAiIiIiNwOAyAiIiJyO/8Pic1h8enwPgYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load trainer log history\n",
    "logs = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "# Training & Eval loss\n",
    "plt.plot(logs[\"step\"], logs[\"loss\"], label=\"Training Loss\", alpha=0.7)\n",
    "if \"eval_loss\" in logs:\n",
    "    plt.plot(logs[\"step\"], logs[\"eval_loss\"], label=\"Eval Loss\", alpha=0.7)\n",
    "\n",
    "# Accuracy\n",
    "if \"accuracy\" in logs:\n",
    "    plt.plot(logs[\"step\"], logs[\"accuracy\"], label=\"Eval Accuracy\", alpha=0.7)\n",
    "\n",
    "# WER\n",
    "if \"wer\" in logs:\n",
    "    plt.plot(logs[\"step\"], logs[\"wer\"], label=\"WER\", alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Metric\")\n",
    "plt.title(\"Training Progress\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8f3db",
   "metadata": {},
   "source": [
    "# Step 7: Save the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80062d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 7: Saving Final Model ---\n",
      "Training complete! Model saved in: ./kapampangan_wav2vec2_model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 7: Saving Final Model ---\")\n",
    "trainer.save_model(MODEL_OUTPUT_DIR)\n",
    "print(f\"Training complete! Model saved in: {MODEL_OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd6d3e",
   "metadata": {},
   "source": [
    "# Step 8: Evaluate on Evaluation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8749dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 8: Evaluating on Validation Set ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 249/249 [00:59<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation WER: 0.7583\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 8: Evaluating on Validation Set ---\")\n",
    "\n",
    "# Optionally reload model\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(MODEL_OUTPUT_DIR).to(device)\n",
    "\n",
    "predictions = trainer.predict(processed_eval_dataset)\n",
    "metrics = compute_metrics(predictions)\n",
    "\n",
    "\n",
    "print(f\"Validation WER: {metrics['wer']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f8fc7",
   "metadata": {},
   "source": [
    "# Show Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb601570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample Predictions ---\n",
      "[328]\n",
      "Kapampangan (Predicted) : [UNK]oka ring pipanganan ing puuntalan mu[UNK] neng bisa kang magan maparasa pamangan[UNK]w\n",
      "Kapampangan (Reference) : [UNK]okaring pipanganan ing pupuntalan mu neng bisa kang mangang maparas a pamangan[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]\n",
      "----------------------------------------\n",
      "[58]\n",
      "Kapampangan (Predicted) : [UNK]aralas kaming mamult basura ban[UNK] makasaup keng pamagli ing ap keng kapaligiran[UNK]w\n",
      "Kapampangan (Reference) : [UNK]aralas kaming mamulut basura ban makasaup keng pamaglingap keng kapaligiran[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]\n",
      "----------------------------------------\n",
      "[13]\n",
      "Kapampangan (Predicted) : [UNK]tatang ku metung ya kareng rol model ku[UNK] [UNK]akalya tiru kakutung kul keng pamagsumikap at pamagkum baba[UNK]w\n",
      "Kapampangan (Reference) : [UNK] tatang ku metung ya kareng role model ku[UNK] [UNK]akal ya tiru kaku tungkul keng pamagsumikap at pamagkumbaba[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]\n",
      "----------------------------------------\n",
      "[380]\n",
      "Kapampangan (Predicted) : [UNK]asanting daramdaman[UNK]sana ikit ke mu naman[UNK]w\n",
      "Kapampangan (Reference) : [UNK]asanting daramdaman[UNK] [UNK]ana ikit ke mu naman[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]\n",
      "----------------------------------------\n",
      "[141]\n",
      "Kapampangan (Predicted) : [UNK]ayap abakk[UNK]w\n",
      "Kapampangan (Reference) : [UNK]ayap a abak[UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK][UNK]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "\n",
    "print(\"\\n--- Sample Predictions ---\")\n",
    "pred_ids = torch.argmax(torch.from_numpy(predictions.predictions), dim=-1)\n",
    "decoded_preds = processor.batch_decode(pred_ids)\n",
    "decoded_labels = processor.batch_decode(predictions.label_ids, group_tokens=False)\n",
    "\n",
    "for i in sample(range(len(decoded_preds)), 5):\n",
    "    print(f\"[{i+1}]\")\n",
    "    print(f\"Kapampangan (Predicted) : {decoded_preds[i]}\")\n",
    "    print(f\"Kapampangan (Reference) : {decoded_labels[i]}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c2db497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import torchaudio\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC\n",
    ")\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.trainer import Trainer\n",
    "\n",
    "# Check for CUDA availability and print device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f2da24",
   "metadata": {},
   "source": [
    "# --- Configuration ---\n",
    "# IMPORTANT: Update these paths for Kapampangan training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cec59b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATED_DATA_FOLDER = 'data/Cleaned_Audio_Files'  # The folder with Kapampangan audio and transcriptions\n",
    "MODEL_OUTPUT_DIR = './kapampangan_wav2vec2_model'  # Directory to save the trained model\n",
    "BASE_MODEL = \"facebook/wav2vec2-large-xlsr-53\"  # Base model for fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252d84e",
   "metadata": {},
   "source": [
    "# --- 1. Load the Dataset ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebad40dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_custom_dataset(data_folder):\n",
    "    \"\"\"Loads the dataset from the metadata.csv file.\"\"\"\n",
    "    metadata_path = os.path.join(data_folder, \"metadata.csv\")\n",
    "    if not os.path.exists(metadata_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"metadata.csv not found in {data_folder}. \"\n",
    "            \"Please ensure you have run the prepare_kapampangan_dataset.py script first.\"\n",
    "        )\n",
    "    dataset_df = pd.read_csv(metadata_path)\n",
    "    # Convert DataFrame to Hugging Face Dataset object\n",
    "    custom_dataset = Dataset.from_pandas(dataset_df)\n",
    "    return custom_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3782449",
   "metadata": {},
   "source": [
    "# --- 2. Create Vocabulary for Kapampangan ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36d70721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(data):\n",
    "    \"\"\"\n",
    "    Extracts all unique characters from the Kapampangan transcription column\n",
    "    and creates a vocabulary file.\n",
    "    \"\"\"\n",
    "    # Regex to extract characters, handling potential variations\n",
    "    chars_to_ignore_regex = r\"[\\,\\?\\.\\!\\-\\;\\:\\\"'%\\[\\]]\"\n",
    "\n",
    "    def extract_all_chars(batch):\n",
    "        all_text = \" \".join(batch[\"transcription\"])\n",
    "        # Normalize and remove special characters\n",
    "        all_text = re.sub(chars_to_ignore_regex, '', all_text).lower()\n",
    "        # Create a set of unique characters\n",
    "        vocab = list(set(all_text))\n",
    "        return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "    # Extract vocabulary from the dataset\n",
    "    vocab_result = data.map(\n",
    "        extract_all_chars,\n",
    "        batched=True,\n",
    "        batch_size=-1,\n",
    "        keep_in_memory=True,\n",
    "        remove_columns=data.column_names\n",
    "    )\n",
    "\n",
    "    # Combine all unique characters from all batches\n",
    "    vocab_list = list(set(vocab_result[\"vocab\"][0]))\n",
    "    vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "\n",
    "    # Add special tokens for CTC loss\n",
    "    vocab_dict[\"|\"] = vocab_dict.pop(\" \") if \" \" in vocab_dict else len(vocab_dict)\n",
    "    vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "    vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "    \n",
    "    # Save the vocabulary as a json file\n",
    "    vocab_path = os.path.join(MODEL_OUTPUT_DIR, 'vocab.json')\n",
    "    if not os.path.exists(MODEL_OUTPUT_DIR):\n",
    "        os.makedirs(MODEL_OUTPUT_DIR)\n",
    "    with open(vocab_path, 'w') as vocab_file:\n",
    "        json.dump(vocab_dict, vocab_file)\n",
    "    \n",
    "    print(f\"Vocabulary created and saved to {vocab_path}\")\n",
    "    print(f\"Vocabulary size: {len(vocab_dict)}\")\n",
    "    return vocab_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6b5c05",
   "metadata": {},
   "source": [
    "# --- 3. Preprocess the Data ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96ca01d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, processor):\n",
    "    \"\"\"\n",
    "    Prepares the dataset for training:\n",
    "    1. Loads and resamples audio.\n",
    "    2. Tokenizes Kapampangan transcriptions.\n",
    "    \"\"\"\n",
    "    import librosa\n",
    "    import soundfile as sf\n",
    "\n",
    "    total_before = len(dataset)\n",
    "    \n",
    "    def prepare_dataset(batch):\n",
    "        try:\n",
    "            audio_path = batch[\"file_path\"]\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "            if sr != 16000:\n",
    "                resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "                waveform = resampler(waveform)\n",
    "\n",
    "            # Convert waveform tensor to numpy array and flatten\n",
    "            audio_array = waveform.squeeze().numpy()\n",
    "\n",
    "            batch[\"input_values\"] = processor(audio_array, sampling_rate=16000).input_values[0]\n",
    "            batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "            with processor.as_target_processor():\n",
    "                batch[\"labels\"] = processor(batch[\"transcription\"]).input_ids\n",
    "            return batch\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Failed to Process\")\n",
    "            print(f\"File: {batch.get('file_path', 'Path not found')}\")\n",
    "            print(f\"Error: {repr(e)}\")  # This will give the actual exception message\n",
    "            print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            return None\n",
    "\n",
    "    # Process each example individually\n",
    "    processed_examples = []\n",
    "    for i in range(len(dataset)):\n",
    "        example = dataset[i]\n",
    "        processed_example = prepare_dataset(example)\n",
    "        if processed_example is not None:\n",
    "            processed_examples.append(processed_example)\n",
    "    \n",
    "    # Create a new dataset from processed examples\n",
    "    from datasets import Dataset\n",
    "    dataset = Dataset.from_list(processed_examples)\n",
    "    total_after = len(dataset)\n",
    "    print(f\"Preprocessing complete: {total_after} / {total_before} samples successfully processed.\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46646bb9",
   "metadata": {},
   "source": [
    "# --- 4. Define Metrics and Data Collator ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b807b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that dynamically pads the inputs and labels for CTC.\n",
    "    \"\"\"\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "        self.padding = \"longest\"\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def compute_metrics(pred, processor, wer_metric):\n",
    "    \"\"\"Computes Word Error Rate (WER) for evaluation.\"\"\"\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = torch.argmax(torch.from_numpy(pred_logits), dim=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids)\n",
    "    label_str = processor.tokenizer.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91789aa",
   "metadata": {},
   "source": [
    "# --- Main Training Execution ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fce35d",
   "metadata": {},
   "source": [
    "# Step 1: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a0aede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Kapampangan Dataset ---\n",
      "Dataset split into 1990 training samples and 498 evaluation samples.\n"
     ]
    }
   ],
   "source": [
    "# Load custom dataset\n",
    "print(\"--- Step 1: Loading Kapampangan Dataset ---\")\n",
    "raw_dataset = load_custom_dataset(VALIDATED_DATA_FOLDER)\n",
    "\n",
    "from datasets import DatasetDict\n",
    "RANDOM_SEED = 42\n",
    "SPLIT_RATIO = 0.2  # 20% for evaluation\n",
    "\n",
    "if len(raw_dataset) > 1:\n",
    "    dataset_split = raw_dataset.train_test_split(\n",
    "        test_size=SPLIT_RATIO,\n",
    "        shuffle=True,\n",
    "        seed=RANDOM_SEED\n",
    "    )\n",
    "    train_dataset = dataset_split['train']\n",
    "    eval_dataset = dataset_split['test']\n",
    "    print(f\"Dataset split into {len(train_dataset)} training samples and {len(eval_dataset)} evaluation samples.\")\n",
    "else:\n",
    "    train_dataset = raw_dataset\n",
    "    eval_dataset = raw_dataset\n",
    "    print(\"Warning: Dataset is too small for a split. Evaluating on the training set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b6cf3",
   "metadata": {},
   "source": [
    "# Step 2: Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50ff88b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Creating Kapampangan Vocabulary ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1990/1990 [00:00<00:00, 142153.16 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary created and saved to ./kapampangan_wav2vec2_model\\vocab.json\n",
      "Vocabulary size: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 2: Creating Kapampangan Vocabulary ---\")\n",
    "vocab_path = create_vocabulary(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f8600",
   "metadata": {},
   "source": [
    "# Step 3: Setup Processor (Tokenizer + Feature Extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5957d263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Setting up Processor ---\n",
      "Processor created and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 3: Setting up Processor ---\")\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\n",
    "    vocab_file=vocab_path,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    word_delimiter_token=\"|\"\n",
    ")\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size=1,\n",
    "    sampling_rate=16000,\n",
    "    padding_value=0.0,\n",
    "    do_normalize=True,\n",
    "    return_attention_mask=False\n",
    ")\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "# Save for reuse\n",
    "processor.save_pretrained(MODEL_OUTPUT_DIR)\n",
    "print(\"Processor created and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1908391",
   "metadata": {},
   "source": [
    "# Step 4: Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7660940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Preprocessing Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete: 1990 / 1990 samples successfully processed.\n",
      "Preprocessing complete: 498 / 498 samples successfully processed.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 4: Preprocessing Data ---\")\n",
    "processed_train_dataset = preprocess_data(train_dataset, processor)\n",
    "processed_eval_dataset = preprocess_data(eval_dataset, processor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f83356d",
   "metadata": {},
   "source": [
    "# Step 5: Setup Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616c063f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1167958232.py, line 103)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 103\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mcallbacks=[EarlyStoppingCallback(early_stopping_patience=3)\u001b[39m\n               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, TrainerCallback\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "print(\"\\n--- Step 5: Setting up Model and Trainer ---\")\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor)\n",
    "\n",
    "# Load metrics\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    vocab_size=len(tokenizer)\n",
    ")\n",
    "model.freeze_feature_encoder()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_DIR,\n",
    "    gradient_accumulation_steps=4,\n",
    "    group_by_length=True,\n",
    "    length_column_name=\"input_length\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=20,\n",
    "    save_strategy=\"steps\",              \n",
    "    evaluation_strategy=\"steps\",      \n",
    "    save_steps=200,                     \n",
    "    eval_steps=200,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,                   \n",
    "    save_total_limit=3,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=200,\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    \n",
    ")\n",
    "# Custom accuracy function (exact string match)\n",
    "def compute_metrics_wrapper(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    # Decode predictions\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Decode labels\n",
    "    label_ids = pred.label_ids\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
    "\n",
    "    # Compute WER\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    # Compute exact-match accuracy\n",
    "    correct = sum([p.strip() == r.strip() for p, r in zip(pred_str, label_str)])\n",
    "    acc = correct / len(label_str) if len(label_str) > 0 else 0.0\n",
    "\n",
    "    return {\"wer\": wer, \"accuracy\": acc}\n",
    "\n",
    "console = Console()\n",
    "\n",
    "class TableLoggerCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # logs can be None if Trainer skips logging\n",
    "        if logs is None:\n",
    "            return\n",
    "\n",
    "        # only print useful keys (ignore \"epoch\", \"total_flos\", etc.)\n",
    "        keys_to_show = [\"loss\", \"learning_rate\", \"eval_loss\", \"wer\"]\n",
    "        logs_to_show = {k: v for k, v in logs.items() if k in keys_to_show}\n",
    "\n",
    "        if not logs_to_show:\n",
    "            return\n",
    "\n",
    "        table = Table(title=f\"Step {state.global_step}\")\n",
    "        for key in logs_to_show.keys():\n",
    "            table.add_column(key, justify=\"center\")\n",
    "\n",
    "        table.add_row(*[\n",
    "            f\"{v:.4f}\" if isinstance(v, (float, int)) else str(v)\n",
    "            for v in logs_to_show.values()\n",
    "        ])\n",
    "\n",
    "        console.print(table)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics_wrapper,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    eval_dataset=processed_eval_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3), early_stopping_threshold=0.01,\n",
    "               TableLoggerCallback()],\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f256fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch ver: 2.6.0+cu124\n",
      "Cuda avail: True\n",
      "Device: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Pytorch ver: {torch.__version__}\")\n",
    "print(f\"Cuda avail: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b7580",
   "metadata": {},
   "source": [
    "# Step 6: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25ceff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 6: Starting Kapampangan Training ---\n",
      "This may take some time depending on your machine and dataset size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:157: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Step 6: Starting Kapampangan Training ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThis may take some time depending on your machine and dataset size.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\trainer.py:1938\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1937\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2279\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2276\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_begin(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2278\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.accumulate(model):\n\u001b[32m-> \u001b[39m\u001b[32m2279\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2282\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2283\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2284\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2285\u001b[39m ):\n\u001b[32m   2286\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2287\u001b[39m     tr_loss += tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3318\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs)\u001b[39m\n\u001b[32m   3315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3317\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3318\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3320\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3322\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3323\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3324\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3363\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs)\u001b[39m\n\u001b[32m   3361\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3362\u001b[39m     labels = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3363\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3364\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3365\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3366\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:2228\u001b[39m, in \u001b[36mWav2Vec2ForCTC.forward\u001b[39m\u001b[34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[39m\n\u001b[32m   2225\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m labels.max() >= \u001b[38;5;28mself\u001b[39m.config.vocab_size:\n\u001b[32m   2226\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLabel values must be <= vocab_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.vocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2228\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwav2vec2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2229\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2231\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2232\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2234\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2236\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   2237\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1819\u001b[39m, in \u001b[36mWav2Vec2Model.forward\u001b[39m\u001b[34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1814\u001b[39m     attention_mask = \u001b[38;5;28mself\u001b[39m._get_feature_vector_attention_mask(\n\u001b[32m   1815\u001b[39m         extract_features.shape[\u001b[32m1\u001b[39m], attention_mask, add_adapter=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1816\u001b[39m     )\n\u001b[32m   1818\u001b[39m hidden_states, extract_features = \u001b[38;5;28mself\u001b[39m.feature_projection(extract_features)\n\u001b[32m-> \u001b[39m\u001b[32m1819\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mask_hidden_states\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1820\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_time_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_time_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\n\u001b[32m   1821\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1823\u001b[39m encoder_outputs = \u001b[38;5;28mself\u001b[39m.encoder(\n\u001b[32m   1824\u001b[39m     hidden_states,\n\u001b[32m   1825\u001b[39m     attention_mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1828\u001b[39m     return_dict=return_dict,\n\u001b[32m   1829\u001b[39m )\n\u001b[32m   1831\u001b[39m hidden_states = encoder_outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Joaquin\\Documents\\School\\College 4th Year\\1st Semester\\6CSTUDY2\\Models\\Pamagsalin_Model_Training\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1769\u001b[39m, in \u001b[36mWav2Vec2Model._mask_hidden_states\u001b[39m\u001b[34m(self, hidden_states, mask_time_indices, attention_mask)\u001b[39m\n\u001b[32m   1761\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.mask_time_prob > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1762\u001b[39m     mask_time_indices = _compute_mask_indices(\n\u001b[32m   1763\u001b[39m         (batch_size, sequence_length),\n\u001b[32m   1764\u001b[39m         mask_prob=\u001b[38;5;28mself\u001b[39m.config.mask_time_prob,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1767\u001b[39m         min_masks=\u001b[38;5;28mself\u001b[39m.config.mask_time_min_masks,\n\u001b[32m   1768\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1769\u001b[39m     mask_time_indices = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_time_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1770\u001b[39m     hidden_states[mask_time_indices] = \u001b[38;5;28mself\u001b[39m.masked_spec_embed.to(hidden_states.dtype)\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.mask_feature_prob > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1773\u001b[39m     \u001b[38;5;66;03m# generate indices & apply SpecAugment along feature axis\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 6: Starting Kapampangan Training ---\")\n",
    "print(\"This may take some time depending on your machine and dataset size.\")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8f3db",
   "metadata": {},
   "source": [
    "# Step 7: Save the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80062d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 7: Saving Final Model ---\")\n",
    "trainer.save_model(MODEL_OUTPUT_DIR)\n",
    "print(f\"Training complete! Model saved in: {MODEL_OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd6d3e",
   "metadata": {},
   "source": [
    "# Step 8: Evaluate on Evaluation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8749dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 8: Evaluating on Validation Set ---\")\n",
    "\n",
    "# Optionally reload model\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(MODEL_OUTPUT_DIR).to(device)\n",
    "\n",
    "predictions = trainer.predict(processed_eval_dataset)\n",
    "metrics = compute_metrics_wrapper(predictions)\n",
    "\n",
    "print(f\"Validation WER: {metrics['wer']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f8fc7",
   "metadata": {},
   "source": [
    "# Show Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb601570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "print(\"\\n--- Sample Predictions ---\")\n",
    "pred_ids = torch.argmax(torch.from_numpy(predictions.predictions), dim=-1)\n",
    "decoded_preds = processor.batch_decode(pred_ids)\n",
    "decoded_labels = processor.batch_decode(predictions.label_ids, group_tokens=False)\n",
    "\n",
    "for i in sample(range(len(decoded_preds)), 5):\n",
    "    print(f\"[{i+1}]\")\n",
    "    print(f\"Kapampangan (Predicted) : {decoded_preds[i]}\")\n",
    "    print(f\"Kapampangan (Reference) : {decoded_labels[i]}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

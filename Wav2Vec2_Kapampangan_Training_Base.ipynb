{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2db497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: CPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers.trainer import Trainer\n",
    "\n",
    "# Check for CUDA availability and print device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f2da24",
   "metadata": {},
   "source": [
    "# --- Configuration ---\n",
    "# IMPORTANT: Update these paths for Kapampangan training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cec59b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATED_DATA_FOLDER = 'data/Cleaned_Audio_Files'  # The folder with Kapampangan audio and transcriptions\n",
    "MODEL_OUTPUT_DIR = \"./kapampangan_wav2vec2_model\"  # Directory to save the trained model\n",
    "BASE_MODEL = \"patrickvonplaten/wav2vec2-base-10m-voxpopuli\" # Base model for fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252d84e",
   "metadata": {},
   "source": [
    "# --- 1. Load the Dataset ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebad40dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_custom_dataset(data_folder):\n",
    "    \"\"\"Loads the dataset from the metadata.csv file into a Hugging Face Dataset object.\"\"\"\n",
    "    metadata_path = os.path.join(data_folder, \"metadata.csv\")\n",
    "    if not os.path.exists(metadata_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"metadata.csv not found in {data_folder}. \"\n",
    "        )\n",
    "    \n",
    "    dataset_df = pd.read_csv(metadata_path)\n",
    "\n",
    "    # Convert DataFrame to Hugging Face Dataset object (raw, no preprocessing here)\n",
    "    custom_dataset = Dataset.from_pandas(dataset_df)\n",
    "    return custom_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3782449",
   "metadata": {},
   "source": [
    "# --- 2. Create Vocabulary for Kapampangan ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d70721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(data):\n",
    "    \"\"\"\n",
    "    Extracts all unique characters from the Kapampangan transcription column\n",
    "    and creates a vocabulary file.\n",
    "    \"\"\"\n",
    "    # Regex to extract characters, handling potential variations\n",
    "    chars_to_ignore_regex = r\"[\\,\\?\\.\\!\\-\\;\\:\\\"'%\\[\\]]\"\n",
    "\n",
    "    def extract_all_chars(batch):\n",
    "        all_text = \" \".join(batch[\"transcription\"])\n",
    "        # Normalize and remove special characters\n",
    "        all_text = re.sub(chars_to_ignore_regex, '', all_text).lower()\n",
    "        # Create a set of unique characters\n",
    "        vocab = list(set(all_text))\n",
    "        return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "    # Extract vocabulary from the dataset\n",
    "    vocab_result = data.map(\n",
    "        extract_all_chars,\n",
    "        batched=True,\n",
    "        batch_size=-1,\n",
    "        keep_in_memory=True,\n",
    "        remove_columns=data.column_names\n",
    "    )\n",
    "\n",
    "    # Combine all unique characters from all batches\n",
    "    vocab_list = list(set(vocab_result[\"vocab\"][0]))\n",
    "    vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "\n",
    "    # Add special tokens for CTC loss\n",
    "    vocab_dict[\"|\"] = vocab_dict.pop(\" \") if \" \" in vocab_dict else len(vocab_dict)\n",
    "    vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "    vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "    \n",
    "    # Save the vocabulary as a json file\n",
    "    vocab_path = os.path.join(MODEL_OUTPUT_DIR, 'vocab.json')\n",
    "    if not os.path.exists(MODEL_OUTPUT_DIR):\n",
    "        os.makedirs(MODEL_OUTPUT_DIR)\n",
    "    with open(vocab_path, 'w') as vocab_file:\n",
    "        json.dump(vocab_dict, vocab_file)\n",
    "    \n",
    "    print(f\"Vocabulary created and saved to {vocab_path}\")\n",
    "    print(f\"Vocabulary size: {len(vocab_dict)}\")\n",
    "    return vocab_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6b5c05",
   "metadata": {},
   "source": [
    "# --- 3. Preprocess the Data ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ca01d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_augment_freq = torchaudio.transforms.FrequencyMasking(freq_mask_param=30)\n",
    "spec_augment_time = torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    "\n",
    "def preprocess_data(dataset, processor):\n",
    "    import librosa\n",
    "    import soundfile as sf\n",
    "\n",
    "    total_before = len(dataset)\n",
    "\n",
    "    def prepare_dataset(batch):\n",
    "        try:\n",
    "            audio_path = batch[\"file_path\"]\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "            if sr != 16000:\n",
    "                resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "                waveform = resampler(waveform)\n",
    "\n",
    "            # ✅ apply SpecAugment on spectrogram\n",
    "            spec = torchaudio.transforms.MelSpectrogram()(waveform)\n",
    "            spec = spec_augment_freq(spec)\n",
    "            spec = spec_augment_time(spec)\n",
    "\n",
    "            # Convert spectrogram back to waveform approximation\n",
    "            # (keeps pipeline consistent with feature extractor)\n",
    "            audio_array = waveform.squeeze().numpy()\n",
    "\n",
    "            batch[\"input_values\"] = processor(audio_array, sampling_rate=16000).input_values[0]\n",
    "            batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "            with processor.as_target_processor():\n",
    "                batch[\"labels\"] = processor(batch[\"transcription\"].lower()).input_ids  # ✅ match vocab\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Failed to Process\")\n",
    "            print(f\"File: {batch.get('file_path', 'Path not found')}\")\n",
    "            print(f\"Error: {repr(e)}\")\n",
    "            print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            return None\n",
    "\n",
    "    processed_examples = []\n",
    "    for i in range(len(dataset)):\n",
    "        example = dataset[i]\n",
    "        processed_example = prepare_dataset(example)\n",
    "        if processed_example is not None:\n",
    "            processed_examples.append(processed_example)\n",
    "\n",
    "    from datasets import Dataset\n",
    "    dataset = Dataset.from_list(processed_examples)\n",
    "    total_after = len(dataset)\n",
    "    print(f\"Preprocessing complete: {total_after} / {total_before} samples successfully processed.\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46646bb9",
   "metadata": {},
   "source": [
    "# --- 4. Define Metrics and Data Collator ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b807b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that dynamically pads the inputs and labels for CTC.\n",
    "    \"\"\"\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "        self.padding = \"longest\"\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def compute_metrics(pred, processor, wer_metric):\n",
    "    \"\"\"Computes Word Error Rate (WER) for evaluation.\"\"\"\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = torch.argmax(torch.from_numpy(pred_logits), dim=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids)\n",
    "    label_str = processor.tokenizer.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91789aa",
   "metadata": {},
   "source": [
    "# --- Main Training Execution ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fce35d",
   "metadata": {},
   "source": [
    "# Step 1: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99a0aede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Kapampangan Dataset ---\n",
      "Dataset split into 1990 training samples and 498 evaluation samples.\n"
     ]
    }
   ],
   "source": [
    "# Load custom dataset\n",
    "print(\"--- Step 1: Loading Kapampangan Dataset ---\")\n",
    "raw_dataset = load_custom_dataset(VALIDATED_DATA_FOLDER)\n",
    "\n",
    "from datasets import DatasetDict\n",
    "RANDOM_SEED = 42\n",
    "SPLIT_RATIO = 0.2  # 20% for evaluation\n",
    "\n",
    "if len(raw_dataset) > 1:\n",
    "    dataset_split = raw_dataset.train_test_split(\n",
    "        test_size=SPLIT_RATIO,\n",
    "        shuffle=True,\n",
    "        seed=RANDOM_SEED\n",
    "    )\n",
    "    train_dataset = dataset_split['train']\n",
    "    eval_dataset = dataset_split['test']\n",
    "    print(f\"Dataset split into {len(train_dataset)} training samples and {len(eval_dataset)} evaluation samples.\")\n",
    "else:\n",
    "    train_dataset = raw_dataset\n",
    "    eval_dataset = raw_dataset\n",
    "    print(\"Warning: Dataset is too small for a split. Evaluating on the training set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b6cf3",
   "metadata": {},
   "source": [
    "# Step 2: Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50ff88b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Creating Kapampangan Vocabulary ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35508e85d08648c091f9aa73697b0ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary created and saved to ./kapampangan_wav2vec2_model\\vocab.json\n",
      "Vocabulary size: 31\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 2: Creating Kapampangan Vocabulary ---\")\n",
    "vocab_path = create_vocabulary(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f8600",
   "metadata": {},
   "source": [
    "# Step 3: Setup Processor (Tokenizer + Feature Extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5957d263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Setting up Processor ---\n",
      "Processor created and saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 3: Setting up Processor ---\")\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\n",
    "    vocab_file=vocab_path,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    word_delimiter_token=\"|\"\n",
    ")\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size=1,\n",
    "    sampling_rate=16000,\n",
    "    padding_value=0.0,\n",
    "    do_normalize=True,\n",
    "    return_attention_mask=False\n",
    ")\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "# Save for reuse\n",
    "processor.save_pretrained(MODEL_OUTPUT_DIR)\n",
    "print(\"Processor created and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1908391",
   "metadata": {},
   "source": [
    "# Step 4: Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7660940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Preprocessing Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n",
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:188: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry014_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry014_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry017_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry017_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry004_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry004_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry002_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry002_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry012_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry012_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry013_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry013_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry005_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry005_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry002_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry002_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry012_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry012_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry015_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry015_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry017_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry017_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry012_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry012_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry015_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry015_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry018_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry018_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry020_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry020_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry018_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry018_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry019_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry019_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry011_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry011_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry001_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry001_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry004_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry004_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry010_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry010_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(032) Jona/cat15_entry028_spk032.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(032) Jona/cat15_entry028_spk032.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry001_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry001_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry005_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry005_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry010_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry010_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry020_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry020_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry009_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry009_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry011_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry011_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry004_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry004_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry014_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry014_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry016_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry016_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry003_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry003_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry020_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry020_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry015_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry015_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry009_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry009_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry001_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry001_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry003_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry003_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry017_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry017_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry010_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry010_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry009_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry009_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry011_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry011_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry016_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry016_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry005_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry005_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry007_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry007_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry003_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry003_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry008_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry008_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry008_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry008_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry007_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry007_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry019_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry019_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry002_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry002_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry019_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry019_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry014_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry014_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry016_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry016_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(012) Lenlen/cat07_entry007_spk012.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(012) Lenlen/cat07_entry007_spk012.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Preprocessing complete: 1936 / 1990 samples successfully processed.\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry013_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry013_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry008_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry008_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry006_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry006_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat02_entry006_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat02_entry006_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat01_entry018_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat01_entry018_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry013_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry013_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry007_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry007_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Failed to Process\n",
      "File: data/Cleaned_Audio_Files/(016) Eya/cat03_entry006_spk016.wav\n",
      "Error: LibsndfileError(2, \"Error opening 'data/Cleaned_Audio_Files/(016) Eya/cat03_entry006_spk016.wav': \")\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Preprocessing complete: 490 / 498 samples successfully processed.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 4: Preprocessing Data ---\")\n",
    "processed_train_dataset = preprocess_data(train_dataset, processor)\n",
    "processed_eval_dataset = preprocess_data(eval_dataset, processor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f83356d",
   "metadata": {},
   "source": [
    "# Step 5: Setup Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616c063f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 5: Setting up Model and Trainer ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\configuration_utils.py:334: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m wer_metric = evaluate.load(\u001b[33m\"\u001b[39m\u001b[33mwer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m acc_metric = evaluate.load(\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m model = \u001b[43mWav2Vec2ForCTC\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBASE_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mctc_loss_reduction\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m model.freeze_feature_encoder()\n\u001b[32m     20\u001b[39m training_args = TrainingArguments(\n\u001b[32m     21\u001b[39m     output_dir=MODEL_OUTPUT_DIR,\n\u001b[32m     22\u001b[39m     gradient_accumulation_steps=\u001b[32m2\u001b[39m,     \u001b[38;5;66;03m# ✅ faster convergence\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     greater_is_better=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     41\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:317\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    315\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:5074\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5064\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5065\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   5067\u001b[39m     (\n\u001b[32m   5068\u001b[39m         model,\n\u001b[32m   5069\u001b[39m         missing_keys,\n\u001b[32m   5070\u001b[39m         unexpected_keys,\n\u001b[32m   5071\u001b[39m         mismatched_keys,\n\u001b[32m   5072\u001b[39m         offload_index,\n\u001b[32m   5073\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m5074\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5075\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5076\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5077\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5078\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5079\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5080\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5081\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5082\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5083\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5084\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5085\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5086\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5087\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5088\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5089\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5090\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5091\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5092\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:5340\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5337\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(state_dict.keys())\n\u001b[32m   5338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   5339\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m-> \u001b[39m\u001b[32m5340\u001b[39m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m.keys()\n\u001b[32m   5341\u001b[39m     )\n\u001b[32m   5343\u001b[39m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[32m   5344\u001b[39m prefix = model.base_model_prefix\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:562\u001b[39m, in \u001b[36mload_state_dict\u001b[39m\u001b[34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[39m\n\u001b[32m    560\u001b[39m \u001b[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001b[39;00m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    564\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\import_utils.py:1622\u001b[39m, in \u001b[36mcheck_torch_load_is_safe\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1620\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_torch_load_is_safe\u001b[39m():\n\u001b[32m   1621\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[33m\"\u001b[39m\u001b[33m2.6\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1622\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1624\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1625\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwhen loading files with safetensors.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1626\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1627\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb460339cce64da7b35ec83fabcecf2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PC\\.cache\\huggingface\\hub\\models--facebook--wav2vec2-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "import evaluate\n",
    "\n",
    "print(\"\\n--- Step 5: Setting up Model and Trainer ---\")\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor)\n",
    "\n",
    "# Load metrics\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    vocab_size=len(tokenizer),\n",
    "    ignore_mismatched_sizes=True  \n",
    ")\n",
    "\n",
    "\n",
    "model.freeze_feature_encoder()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_OUTPUT_DIR,\n",
    "    gradient_accumulation_steps=2,     # ✅ faster convergence\n",
    "    group_by_length=True,\n",
    "    length_column_name=\"input_length\",\n",
    "    per_device_train_batch_size=8,     # ✅ fits in 8–12GB VRAM with fp16\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=20,               # ✅ more epochs, smaller LR\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    fp16=True,                         # ✅ mixed precision for NVIDIA 4070\n",
    "    gradient_checkpointing=True,       # ✅ saves memory\n",
    "    logging_steps=25,\n",
    "    learning_rate=3e-5,                # ✅ safer LR for fine-tuning\n",
    "    warmup_steps=500,\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Compute both WER & Accuracy\n",
    "cer_metric = evaluate.load(\"cer\")  # load Character Error Rate metric\n",
    "\n",
    "def compute_metrics_wrapper(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids = pred.label_ids\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer, \"cer\": cer}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics_wrapper,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    eval_dataset=processed_eval_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # <-- early stopping\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b7580",
   "metadata": {},
   "source": [
    "# Step 6: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ceff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 6: Starting Kapampangan Training ---\n",
      "This may take some time depending on your machine and dataset size.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a51fd9644e408990104fb14b432e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1452 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Step 6: Starting Kapampangan Training ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThis may take some time depending on your machine and dataset size.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:1938\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   1937\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:2279\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2276\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_begin(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2278\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.accumulate(model):\n\u001b[32m-> \u001b[39m\u001b[32m2279\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2282\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2283\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2284\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2285\u001b[39m ):\n\u001b[32m   2286\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2287\u001b[39m     tr_loss += tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\trainer.py:3349\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3347\u001b[39m         scaled_loss.backward()\n\u001b[32m   3348\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3349\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3351\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach() / \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\accelerate\\accelerator.py:2553\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2551\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2553\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:492\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    484\u001b[39m         Tensor.backward,\n\u001b[32m    485\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m         inputs=inputs,\n\u001b[32m    491\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:251\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    246\u001b[39m     retain_graph = create_graph\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\function.py:288\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    283\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImplementing both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mvjp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for a custom \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    284\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFunction is not allowed. You should only implement one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     )\n\u001b[32m    287\u001b[39m user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\checkpoint.py:271\u001b[39m, in \u001b[36mCheckpointFunction.backward\u001b[39m\u001b[34m(ctx, *args)\u001b[39m\n\u001b[32m    266\u001b[39m     device_autocast_ctx = device_module.amp.autocast(\n\u001b[32m    267\u001b[39m         **ctx.device_autocast_kwargs\n\u001b[32m    268\u001b[39m     ) \u001b[38;5;28;01mif\u001b[39;00m _supports_autocast(ctx.device) \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext()\n\u001b[32m    269\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad(), device_autocast_ctx, \\\n\u001b[32m    270\u001b[39m          torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m         outputs = \u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdetached_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, torch.Tensor):\n\u001b[32m    274\u001b[39m     outputs = (outputs,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1518\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1522\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1523\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1525\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1526\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1527\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1529\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1530\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:943\u001b[39m, in \u001b[36mWav2Vec2EncoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions)\u001b[39m\n\u001b[32m    940\u001b[39m hidden_states = attn_residual + hidden_states\n\u001b[32m    942\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.layer_norm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m hidden_states = hidden_states + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    944\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.final_layer_norm(hidden_states)\n\u001b[32m    946\u001b[39m outputs = (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1518\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1522\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1523\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1525\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1526\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1527\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1529\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1530\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:915\u001b[39m, in \u001b[36mWav2Vec2FeedForward.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    912\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.intermediate_dropout(hidden_states)\n\u001b[32m    914\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.output_dense(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_dropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1518\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1522\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1523\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1525\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1526\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1527\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1529\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1530\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\dropout.py:58\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:1266\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1264\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1265\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1266\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Step 6: Starting Kapampangan Training ---\")\n",
    "print(\"This may take some time depending on your machine and dataset size.\")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8f3db",
   "metadata": {},
   "source": [
    "# Step 7: Save the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80062d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 7: Saving Final Model ---\")\n",
    "trainer.save_model(MODEL_OUTPUT_DIR)\n",
    "print(f\"Training complete! Model saved in: {MODEL_OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd6d3e",
   "metadata": {},
   "source": [
    "# Step 8: Evaluate on Evaluation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8749dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 8: Evaluating on Validation Set ---\")\n",
    "\n",
    "# Optionally reload model\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(MODEL_OUTPUT_DIR).to(device)\n",
    "\n",
    "predictions = trainer.predict(processed_eval_dataset)\n",
    "metrics = compute_metrics_wrapper(predictions)\n",
    "\n",
    "print(f\"Validation WER: {metrics['wer']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f8fc7",
   "metadata": {},
   "source": [
    "# Show Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb601570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "print(\"\\n--- Sample Predictions ---\")\n",
    "pred_ids = torch.argmax(torch.from_numpy(predictions.predictions), dim=-1)\n",
    "decoded_preds = processor.batch_decode(pred_ids)\n",
    "decoded_labels = processor.batch_decode(predictions.label_ids, group_tokens=False)\n",
    "\n",
    "for i in sample(range(len(decoded_preds)), 5):\n",
    "    print(f\"[{i+1}]\")\n",
    "    print(f\"Kapampangan (Predicted) : {decoded_preds[i]}\")\n",
    "    print(f\"Kapampangan (Reference) : {decoded_labels[i]}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
